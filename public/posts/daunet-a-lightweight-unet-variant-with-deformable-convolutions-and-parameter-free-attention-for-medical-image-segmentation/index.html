<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>DAUNet: A Lightweight UNet Variant with Deformable Convolutions and Parameter-Free Attention for Medical Image Segmentation | Research Agent Knowledge Base</title>
<meta name="keywords" content="Deformable Convolutions (변형 가능 컨볼루션), Parameter-Free Attention (매개변수 없는 어텐션), Lightweight UNet (경량 U-Net), Medical Image Segmentation (의료 영상 분할), Real-Time Deployment (실시간 배포)">
<meta name="description" content="Abstract Medical image segmentation plays a pivotal role in automated diagnostic and treatment planning systems. In this work, we present DAUNet, a novel lightweight UNet variant that integrates Deformable V2 Convolutions and Parameter-Free Attention (SimAM) to improve spatial adaptability and context-aware feature fusion without increasing model complexity. DAUNet&rsquo;s bottleneck employs dynamic deformable kernels to handle geometric variations, while the decoder and skip pathways are enhanced using SimAM attention modules for saliency-aware refinement.">
<meta name="author" content="">
<link rel="canonical" href="https://hanwo-ol.github.io/Unet_master/posts/daunet-a-lightweight-unet-variant-with-deformable-convolutions-and-parameter-free-attention-for-medical-image-segmentation/">
<link crossorigin="anonymous" href="/Unet_master/assets/css/stylesheet.62aa25427797f8efd87301a5b69795dc50df2dbe79a5fba0648cc7bb8dbcd7c9.css" integrity="sha256-YqolQneX&#43;O/YcwGltpeV3FDfLb55pfugZIzHu42818k=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/Unet_master/assets/js/highlight.acb54fd32bbc1982428b8850317e45d076b95012730a5936667e6bc21777692a.js" integrity="sha256-rLVP0yu8GYJCi4hQMX5F0Ha5UBJzClk2Zn5rwhd3aSo="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://hanwo-ol.github.io/Unet_master/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://hanwo-ol.github.io/Unet_master/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://hanwo-ol.github.io/Unet_master/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://hanwo-ol.github.io/Unet_master/apple-touch-icon.png">
<link rel="mask-icon" href="https://hanwo-ol.github.io/Unet_master/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasYbI1F/F" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYwHR5IXS0n1q35dx72xvRIe8+y/k6PrmJeCA1pNEk1e" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>

<meta property="og:title" content="DAUNet: A Lightweight UNet Variant with Deformable Convolutions and Parameter-Free Attention for Medical Image Segmentation" />
<meta property="og:description" content="Abstract Medical image segmentation plays a pivotal role in automated diagnostic and treatment planning systems. In this work, we present DAUNet, a novel lightweight UNet variant that integrates Deformable V2 Convolutions and Parameter-Free Attention (SimAM) to improve spatial adaptability and context-aware feature fusion without increasing model complexity. DAUNet&rsquo;s bottleneck employs dynamic deformable kernels to handle geometric variations, while the decoder and skip pathways are enhanced using SimAM attention modules for saliency-aware refinement." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://hanwo-ol.github.io/Unet_master/posts/daunet-a-lightweight-unet-variant-with-deformable-convolutions-and-parameter-free-attention-for-medical-image-segmentation/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2025-12-07T00:00:00+00:00" />
<meta property="article:modified_time" content="2025-12-07T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="DAUNet: A Lightweight UNet Variant with Deformable Convolutions and Parameter-Free Attention for Medical Image Segmentation"/>
<meta name="twitter:description" content="Abstract Medical image segmentation plays a pivotal role in automated diagnostic and treatment planning systems. In this work, we present DAUNet, a novel lightweight UNet variant that integrates Deformable V2 Convolutions and Parameter-Free Attention (SimAM) to improve spatial adaptability and context-aware feature fusion without increasing model complexity. DAUNet&rsquo;s bottleneck employs dynamic deformable kernels to handle geometric variations, while the decoder and skip pathways are enhanced using SimAM attention modules for saliency-aware refinement."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://hanwo-ol.github.io/Unet_master/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "DAUNet: A Lightweight UNet Variant with Deformable Convolutions and Parameter-Free Attention for Medical Image Segmentation",
      "item": "https://hanwo-ol.github.io/Unet_master/posts/daunet-a-lightweight-unet-variant-with-deformable-convolutions-and-parameter-free-attention-for-medical-image-segmentation/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "DAUNet: A Lightweight UNet Variant with Deformable Convolutions and Parameter-Free Attention for Medical Image Segmentation",
  "name": "DAUNet: A Lightweight UNet Variant with Deformable Convolutions and Parameter-Free Attention for Medical Image Segmentation",
  "description": "Abstract Medical image segmentation plays a pivotal role in automated diagnostic and treatment planning systems. In this work, we present DAUNet, a novel lightweight UNet variant that integrates Deformable V2 Convolutions and Parameter-Free Attention (SimAM) to improve spatial adaptability and context-aware feature fusion without increasing model complexity. DAUNet\u0026rsquo;s bottleneck employs dynamic deformable kernels to handle geometric variations, while the decoder and skip pathways are enhanced using SimAM attention modules for saliency-aware refinement.",
  "keywords": [
    "Deformable Convolutions (변형 가능 컨볼루션)", "Parameter-Free Attention (매개변수 없는 어텐션)", "Lightweight UNet (경량 U-Net)", "Medical Image Segmentation (의료 영상 분할)", "Real-Time Deployment (실시간 배포)"
  ],
  "articleBody": "Abstract Medical image segmentation plays a pivotal role in automated diagnostic and treatment planning systems. In this work, we present DAUNet, a novel lightweight UNet variant that integrates Deformable V2 Convolutions and Parameter-Free Attention (SimAM) to improve spatial adaptability and context-aware feature fusion without increasing model complexity. DAUNet’s bottleneck employs dynamic deformable kernels to handle geometric variations, while the decoder and skip pathways are enhanced using SimAM attention modules for saliency-aware refinement. Extensive evaluations on two challenging datasets, FH-PS-AoP (fetal head and pubic symphysis ultrasound) and FUMPE (CT-based pulmonary embolism detection), demonstrate that DAUNet outperforms state-of-the-art models in Dice score, HD95, and ASD, while maintaining superior parameter efficiency. Ablation studies highlight the individual contributions of deformable convolutions and SimAM attention. DAUNet’s robustness to missing context and low-contrast regions establishes its suitability for deployment in real-time and resource-constrained clinical environments.\nPDF Download Local PDF View | Arxiv Original\nResearch Agent - Draft Refiner Module 리포트 1. 요약 (Executive Summary) DAUNet은 의료 영상 분할을 위해 Deformable V2 Convolutions (DCN V2)와 Parameter-Free Attention (SimAM)을 통합한 경량화된 U-Net 변형 아키텍처입니다. 이 연구의 핵심 내용은 다음과 같습니다.\n경량화 및 효율성: DAUNet은 모델의 복잡도를 크게 증가시키지 않으면서 공간 적응성과 문맥 인식 특징 융합 능력을 향상시키는 것을 목표로 합니다. 전체 매개변수 수는 20.47M으로, 기존 SOTA 모델 대비 현저히 낮습니다. 동적 공간 적응성: 병목(Bottleneck) 블록에 DCN V2를 도입하여 동적이고 공간적으로 적응 가능한 수용장(receptive field)을 생성합니다. 이는 불규칙한 해부학적 경계 및 기하학적 변형을 효과적으로 포착할 수 있게 합니다. 매개변수 없는 특징 정제: 디코더 블록과 스킵 연결 경로에 SimAM(Simple Attention Module)을 통합하여, 추가적인 학습 가능한 매개변수 없이도 중요 영역을 강조하고 특징 융합을 개선합니다. 우수한 성능 및 강건성: FH-PS-AoP (초음파) 및 FUMPE (CT 혈관조영술) 두 가지 도전적인 의료 영상 데이터셋에서 기존 SOTA 모델 대비 Dice Score, HD95, ASD 측면에서 우수한 성능을 달성했습니다. 임상 적용 가능성: 누락된 문맥(missing context) 및 저대비 영역에 대한 강건성을 입증하여, 실시간 처리 및 자원 제약적인 임상 환경(예: 모바일 초음파 시스템)에 배포하기에 적합함을 보여줍니다. 2. 7가지 핵심 질문 분석 (Key Analysis) 1) What is new in the work? (기존 연구와의 차별점) DAUNet은 기존 U-Net 아키텍처에 두 가지 핵심 혁신을 통합하여 경량화된 변형 모델을 제안합니다. 첫째, 병목 블록에 Modulated Deformable V2 Convolutions을 사용하여 고정된 그리드 컨볼루션의 한계를 극복하고 해부학적 변형에 동적으로 적응합니다. 둘째, 디코더 블록과 스킵 연결 경로에 Parameter-Free Attention (SimAM) 모듈을 통합하여, 모델의 매개변수 수를 늘리지 않으면서도 공간적 특징 표현을 강화하고 특징 융합을 정제합니다. 이러한 조합은 정확도와 계산 효율성 사이의 최적의 균형을 제공합니다.\n2) Why is the work important? (연구의 중요성) 이 연구는 의료 영상 분할 분야에서 정확도와 효율성이라는 상충되는 목표를 동시에 달성했다는 점에서 중요합니다. 특히 초음파나 CT 혈관조영술과 같이 해부학적 가변성이 높고 저대비 영역이 흔한 환경에서, DAUNet은 SOTA 성능을 유지하면서도 매개변수 수를 획기적으로 줄였습니다. 이는 TransUNet (105.28M)이나 SCUNet++ (60.11M)과 같은 무거운 모델들이 실시간 추론 및 자원 제약적인 엣지 디바이스에 배포되기 어려운 한계를 극복하고, 실제 임상 환경에서의 활용 가능성을 높입니다.\n3) What is the literature gap? (기존 연구의 한계점) 기존 U-Net 아키텍처는 고정된 컨볼루션 필드를 사용하여 가변적인 크기의 특징이나 불규칙한 장기 경계를 포착하는 데 제한적입니다. 최근 트랜스포머 기반 하이브리드 모델들은 장거리 의존성 포착을 통해 성능을 개선했지만, 높은 계산 복잡도와 많은 매개변수(Parameter Burden)를 요구하여 추론 속도가 느립니다. 따라서, 높은 정확도를 유지하면서도 실시간 배포가 가능한 경량화되고 적응성 있는 모델에 대한 요구가 존재했습니다.\n4) How is the gap filled? (해결 방안) DAUNet은 기존 U-Net의 인코더-디코더 구조를 유지하면서, 병목 블록에 DCN V2를 적용하여 공간적 적응성을 높이고, 디코더와 스킵 연결에 SimAM을 적용하여 특징의 중요도를 매개변수 없이 학습합니다. DCN V2는 동적 오프셋을 학습하여 기하학적 변형에 대응하며, SimAM은 신경과학 이론에 기반하여 활성화 에너지가 낮은(정보량이 높은) 뉴런에 더 높은 가중치를 부여함으로써 특징 맵을 정제합니다. 이 두 가지 경량화된 모듈의 통합은 모델의 복잡도를 최소화하면서 성능을 극대화합니다.\n5) What is achieved with the new method? (달성한 성과) DAUNet은 두 가지 데이터셋에서 SOTA 성능을 달성했습니다.\n데이터셋 모델 DSC (↑) HD95 (↓) ASD (↓) Param (M) (↓) FH-PS-AoP DAUNet (Proposed) 89.09% 10.37 3.70 20.47 FH-PS-AoP TransUNet 87.34% 13.25 3.67 105.28 FH-PS-AoP UNet (Baseline) 80.22% 15.87 4.88 31.03 FUMPE DAUNet (Proposed) 88.80% 2.57 - 20.47 FUMPE FAT-Net 84.44% 3.67 - 30.00 FH-PS-AoP 데이터셋에서 DAUNet은 평균 DSC 89.09%를 달성하며, TransUNet (105.28M)보다 5배 이상 적은 매개변수(20.47M)로 더 높은 정확도를 보였습니다. 특히 경계 기반 지표인 HD95와 ASD에서 가장 낮은 수치를 기록하여 우수한 경계 정밀도를 입증했습니다. FUMPE 데이터셋에서도 DSC 88.80%로 최고 성능을 달성했습니다.\n6) What data are used? (사용 데이터셋) 두 가지 도전적인 의료 영상 분할 데이터셋이 사용되었습니다.\nFH-PS-AoP (Pubic Symphysis and Fetal Head Detection): 트랜스페리네알 초음파(transperineal ultrasound) 2D B-모드 영상으로 구성됩니다. 태아 머리와 치골 결합(pubic symphysis) 분할을 목표로 하며, 해부학적 가변성이 크고 저대비 환경이 특징입니다. FUMPE (Pulmonary Embolism Detection): CT 혈관조영술(CT angiography, CTA) 3D 스캔 영상으로 구성되며, 폐색전증(PE) 검출을 목표로 합니다. 특히 PE 영역의 약 67%가 말초 폐동맥에 발생하여 정교한 경계 분할이 요구됩니다. 7) What are the limitations? (저자가 언급한 한계점) 저자는 DAUNet의 명시적인 한계점을 언급하기보다는, 현재 모델이 2D 영상에 초점을 맞추고 있음을 시사하며 향후 연구 방향을 제시했습니다. 미래 연구는 다음과 같습니다.\n프레임워크를 멀티모달 및 3D 영상으로 확장. 도메인 적응(domain adaptation)을 통해 교차 도메인 일반화(cross-domain generalization)를 개선. 엣지 디바이스에서의 실시간 추론을 위해 모델을 추가로 최적화. 3. 아키텍처 및 방법론 (Architecture \u0026 Methodology) Figure 분석: DAUNet 아키텍처 (Figure 3) DAUNet은 고전적인 U-Net의 인코더-디코더 구조를 기반으로 하며, 특히 병목 블록과 스킵 연결 경로에 핵심적인 수정 사항을 도입했습니다.\nU-Net 구조에서 변경된 블록 및 흐름:\n인코더 및 디코더 블록: 기존 U-Net과 유사하게 다운스케일링(Downscaling) 및 업스케일링(Upscaling) 경로를 가지며, 각 레벨은 DoubleConv (DC) 블록을 사용합니다. 모든 컨볼루션 연산은 $3 \\times 3$ 필터를 사용합니다. 병목 (Bottleneck) 블록의 재설계: 기존 U-Net의 일반 컨볼루션 대신, DAUNet의 병목은 네 단계의 연속적인 연산으로 구성된 복합 구조를 사용합니다. 채널 압축: $1 \\times 1$ 컨볼루션을 사용하여 입력 채널을 목표 출력 채널의 1/4로 압축하여 계산 비용을 제어합니다. 공간 적응: $3 \\times 3$ Deformable Convolution V2 (DCN V2) 레이어를 적용하여 동적 오프셋을 학습하고 입력 특징의 기하학적 변형을 포착합니다. 채널 복원: 두 번째 $1 \\times 1$ 컨볼루션을 사용하여 특징을 원래 채널 차원으로 다시 투영합니다. 특징 정제: SimAM 모듈을 마지막에 추가하여 에너지 기반 기준으로 공간적으로 정보가 풍부한 활성화를 강조하고 출력을 정제합니다. 스킵 연결 (Skip Connections) 강화: 인코더와 디코더 특징을 병합(Merge)하기 전에, 모든 스킵 연결 경로에 SimAM 모듈이 통합됩니다. 이 SimAM 모듈은 인코더에서 디코더로 전달되는 특징 맵의 관련성이 낮은 활성화를 억제하고 의미적으로 풍부한 특징의 전송을 강화합니다. 수식 상세 1. Modulated Deformable Convolution V2 (변조된 변형 가능 컨볼루션 V2) DCN V2는 표준 컨볼루션의 고정된 샘플링 위치($k$)를 학습 가능한 오프셋($\\Delta p_k$)으로 조정하고, 변조 스칼라($\\alpha_k$)를 도입하여 특정 영역을 선택적으로 강조하거나 억제합니다. 출력 $Y$는 위치 $p$에서 다음과 같이 계산됩니다.\n$$Y(p) = \\sum_{k \\in \\mathcal{R}} \\alpha_k \\cdot K(k) \\cdot F(p + k + \\Delta p_k)$$\n$F$: 입력 특징 맵 $K$: 컨볼루션 커널 $\\mathcal{R}$: 수용장(receptive field) $\\Delta p_k$: 학습 가능한 오프셋 (입력 특징에 따라 동적으로 조정됨) $\\alpha_k$: 변조 스칼라 ($[0, 1]$ 범위, 특정 영역의 중요도를 조절) 2. Parameter-Free Attention: SimAM (Simple Attention Module) SimAM은 신경과학 이론에 영감을 받아, 활성화 에너지가 높은 뉴런이 덜 유익하다는 가정 하에 각 뉴런의 중요도를 에너지 기반 함수를 통해 평가합니다.\n에너지 함수 ($E_t$) (Eq. 3):\n주어진 특징 맵 $X \\in \\mathbb{R}^{C \\times H \\times W}$에서, 위치 $t$의 뉴런에 대한 에너지 $E_t$는 다음과 같습니다.\n$$E_t = (x_t - \\mu_t)^2 + \\frac{1}{\\lambda} \\sum_{i \\neq t} (x_i - \\mu_t)^2$$\n$x_t$: 대상 뉴런의 활성화 값 $\\mu_t$: $x_t$를 제외한 동일 채널 내 모든 뉴런의 평균 $\\lambda$: 주변 뉴런의 중요도를 제어하는 하이퍼파라미터 ($1 \\times 10^{-4}$로 설정됨) 어텐션 가중치 ($a_t$) (Eq. 4):\n각 뉴런에 대한 어텐션 가중치 $a_t$는 에너지 함수를 시그모이드 활성화 함수 $\\sigma(\\cdot)$를 통해 계산됩니다.\n$$a_t = \\frac{1}{\\sigma(\\frac{E_t}{\\epsilon}) + \\epsilon}$$\n$\\epsilon$: 0으로 나누는 것을 방지하는 작은 상수 정제된 출력 ($X’$) (Eq. 5):\n정제된 출력 특징 맵 $X’$는 원래 특징 맵 $X$와 어텐션 맵 $A$의 요소별 곱셈으로 얻어집니다.\n$$X’ = X \\odot A$$\n3. Loss Function (손실 함수) 학습 과정에서는 하이브리드 손실 함수가 사용되었습니다.\n$$\\text{Loss} = \\text{Dice Loss} + \\text{Weighted Binary Cross-Entropy (BCE)}$$\n이 조합은 분할 정확도(Dice Loss)와 클래스 불균형 문제 처리(Weighted BCE)를 동시에 목표로 합니다.\nVanilla U-Net 비교 특징 Vanilla U-Net DAUNet (Proposed) 기본 구조 인코더-디코더, 스킵 연결 인코더-디코더, 스킵 연결 컨볼루션 유형 고정 그리드 컨볼루션 DCN V2 (병목), 표준 컨볼루션 (인코더/디코더) 병목 블록 일반 컨볼루션 레이어 $1 \\times 1$ Conv $\\rightarrow$ DCN V2 $\\rightarrow$ $1 \\times 1$ Conv $\\rightarrow$ SimAM 스킵 연결 인코더 특징을 디코더에 직접 연결 (Concatenation) 연결 전에 SimAM 모듈을 적용하여 특징 정제 후 연결 어텐션 메커니즘 없음 (Attention U-Net 제외) SimAM (Parameter-Free Attention) 사용 매개변수 수 (FH-PS-AoP) 31.03M 20.47M 4. 태그 제안 (Tags Suggestion) Deformable Convolutions (변형 가능 컨볼루션) Parameter-Free Attention (매개변수 없는 어텐션) Lightweight UNet (경량 U-Net) Medical Image Segmentation (의료 영상 분할) Real-Time Deployment (실시간 배포) ",
  "wordCount" : "1362",
  "inLanguage": "en",
  "datePublished": "2025-12-07T00:00:00Z",
  "dateModified": "2025-12-07T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://hanwo-ol.github.io/Unet_master/posts/daunet-a-lightweight-unet-variant-with-deformable-convolutions-and-parameter-free-attention-for-medical-image-segmentation/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Research Agent Knowledge Base",
    "logo": {
      "@type": "ImageObject",
      "url": "https://hanwo-ol.github.io/Unet_master/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://hanwo-ol.github.io/Unet_master/" accesskey="h" title="Research Agent Knowledge Base (Alt + H)">Research Agent Knowledge Base</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://hanwo-ol.github.io/Unet_master/categories/" title="categories">
                    <span>categories</span>
                </a>
            </li>
            <li>
                <a href="https://hanwo-ol.github.io/Unet_master/tags/" title="tags">
                    <span>tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://hanwo-ol.github.io/Unet_master/">Home</a>&nbsp;»&nbsp;<a href="https://hanwo-ol.github.io/Unet_master/posts/">Posts</a></div>
    <h1 class="post-title">
      DAUNet: A Lightweight UNet Variant with Deformable Convolutions and Parameter-Free Attention for Medical Image Segmentation
    </h1>
    <div class="post-meta">&lt;span title=&#39;2025-12-07 00:00:00 &#43;0000 UTC&#39;&gt;December 7, 2025&lt;/span&gt;&amp;nbsp;·&amp;nbsp;7 min

</div>
  </header> 
  <div class="post-content"><h2 id="abstract">Abstract<a hidden class="anchor" aria-hidden="true" href="#abstract">#</a></h2>
<p>Medical image segmentation plays a pivotal role in automated diagnostic and treatment planning systems. In this work, we present DAUNet, a novel lightweight UNet variant that integrates Deformable V2 Convolutions and Parameter-Free Attention (SimAM) to improve spatial adaptability and context-aware feature fusion without increasing model complexity. DAUNet&rsquo;s bottleneck employs dynamic deformable kernels to handle geometric variations, while the decoder and skip pathways are enhanced using SimAM attention modules for saliency-aware refinement. Extensive evaluations on two challenging datasets, FH-PS-AoP (fetal head and pubic symphysis ultrasound) and FUMPE (CT-based pulmonary embolism detection), demonstrate that DAUNet outperforms state-of-the-art models in Dice score, HD95, and ASD, while maintaining superior parameter efficiency. Ablation studies highlight the individual contributions of deformable convolutions and SimAM attention. DAUNet&rsquo;s robustness to missing context and low-contrast regions establishes its suitability for deployment in real-time and resource-constrained clinical environments.</p>
<h2 id="pdf-download">PDF Download<a hidden class="anchor" aria-hidden="true" href="#pdf-download">#</a></h2>
<p><a href="//172.22.138.185/Research_pdf/2512.07051v1.pdf">Local PDF View</a> | <a href="http://arxiv.org/abs/2512.07051v1">Arxiv Original</a></p>
<h2 id="research-agent---draft-refiner-module-리포트">Research Agent - Draft Refiner Module 리포트<a hidden class="anchor" aria-hidden="true" href="#research-agent---draft-refiner-module-리포트">#</a></h2>
<hr>
<h2 id="1-요약-executive-summary">1. 요약 (Executive Summary)<a hidden class="anchor" aria-hidden="true" href="#1-요약-executive-summary">#</a></h2>
<p>DAUNet은 의료 영상 분할을 위해 Deformable V2 Convolutions (DCN V2)와 Parameter-Free Attention (SimAM)을 통합한 경량화된 U-Net 변형 아키텍처입니다. 이 연구의 핵심 내용은 다음과 같습니다.</p>
<ul>
<li><strong>경량화 및 효율성:</strong> DAUNet은 모델의 복잡도를 크게 증가시키지 않으면서 공간 적응성과 문맥 인식 특징 융합 능력을 향상시키는 것을 목표로 합니다. 전체 매개변수 수는 20.47M으로, 기존 SOTA 모델 대비 현저히 낮습니다.</li>
<li><strong>동적 공간 적응성:</strong> 병목(Bottleneck) 블록에 DCN V2를 도입하여 동적이고 공간적으로 적응 가능한 수용장(receptive field)을 생성합니다. 이는 불규칙한 해부학적 경계 및 기하학적 변형을 효과적으로 포착할 수 있게 합니다.</li>
<li><strong>매개변수 없는 특징 정제:</strong> 디코더 블록과 스킵 연결 경로에 SimAM(Simple Attention Module)을 통합하여, 추가적인 학습 가능한 매개변수 없이도 중요 영역을 강조하고 특징 융합을 개선합니다.</li>
<li><strong>우수한 성능 및 강건성:</strong> FH-PS-AoP (초음파) 및 FUMPE (CT 혈관조영술) 두 가지 도전적인 의료 영상 데이터셋에서 기존 SOTA 모델 대비 Dice Score, HD95, ASD 측면에서 우수한 성능을 달성했습니다.</li>
<li><strong>임상 적용 가능성:</strong> 누락된 문맥(missing context) 및 저대비 영역에 대한 강건성을 입증하여, 실시간 처리 및 자원 제약적인 임상 환경(예: 모바일 초음파 시스템)에 배포하기에 적합함을 보여줍니다.</li>
</ul>
<hr>
<h2 id="2-7가지-핵심-질문-분석-key-analysis">2. 7가지 핵심 질문 분석 (Key Analysis)<a hidden class="anchor" aria-hidden="true" href="#2-7가지-핵심-질문-분석-key-analysis">#</a></h2>
<h3 id="1-what-is-new-in-the-work-기존-연구와의-차별점">1) What is new in the work? (기존 연구와의 차별점)<a hidden class="anchor" aria-hidden="true" href="#1-what-is-new-in-the-work-기존-연구와의-차별점">#</a></h3>
<p>DAUNet은 기존 U-Net 아키텍처에 두 가지 핵심 혁신을 통합하여 경량화된 변형 모델을 제안합니다. 첫째, 병목 블록에 <strong>Modulated Deformable V2 Convolutions</strong>을 사용하여 고정된 그리드 컨볼루션의 한계를 극복하고 해부학적 변형에 동적으로 적응합니다. 둘째, 디코더 블록과 스킵 연결 경로에 <strong>Parameter-Free Attention (SimAM)</strong> 모듈을 통합하여, 모델의 매개변수 수를 늘리지 않으면서도 공간적 특징 표현을 강화하고 특징 융합을 정제합니다. 이러한 조합은 정확도와 계산 효율성 사이의 최적의 균형을 제공합니다.</p>
<h3 id="2-why-is-the-work-important-연구의-중요성">2) Why is the work important? (연구의 중요성)<a hidden class="anchor" aria-hidden="true" href="#2-why-is-the-work-important-연구의-중요성">#</a></h3>
<p>이 연구는 의료 영상 분할 분야에서 정확도와 효율성이라는 상충되는 목표를 동시에 달성했다는 점에서 중요합니다. 특히 초음파나 CT 혈관조영술과 같이 해부학적 가변성이 높고 저대비 영역이 흔한 환경에서, DAUNet은 SOTA 성능을 유지하면서도 매개변수 수를 획기적으로 줄였습니다. 이는 TransUNet (105.28M)이나 SCUNet++ (60.11M)과 같은 무거운 모델들이 실시간 추론 및 자원 제약적인 엣지 디바이스에 배포되기 어려운 한계를 극복하고, 실제 임상 환경에서의 활용 가능성을 높입니다.</p>
<h3 id="3-what-is-the-literature-gap-기존-연구의-한계점">3) What is the literature gap? (기존 연구의 한계점)<a hidden class="anchor" aria-hidden="true" href="#3-what-is-the-literature-gap-기존-연구의-한계점">#</a></h3>
<p>기존 U-Net 아키텍처는 고정된 컨볼루션 필드를 사용하여 가변적인 크기의 특징이나 불규칙한 장기 경계를 포착하는 데 제한적입니다. 최근 트랜스포머 기반 하이브리드 모델들은 장거리 의존성 포착을 통해 성능을 개선했지만, 높은 계산 복잡도와 많은 매개변수(Parameter Burden)를 요구하여 추론 속도가 느립니다. 따라서, 높은 정확도를 유지하면서도 실시간 배포가 가능한 경량화되고 적응성 있는 모델에 대한 요구가 존재했습니다.</p>
<h3 id="4-how-is-the-gap-filled-해결-방안">4) How is the gap filled? (해결 방안)<a hidden class="anchor" aria-hidden="true" href="#4-how-is-the-gap-filled-해결-방안">#</a></h3>
<p>DAUNet은 기존 U-Net의 인코더-디코더 구조를 유지하면서, 병목 블록에 DCN V2를 적용하여 공간적 적응성을 높이고, 디코더와 스킵 연결에 SimAM을 적용하여 특징의 중요도를 매개변수 없이 학습합니다. DCN V2는 동적 오프셋을 학습하여 기하학적 변형에 대응하며, SimAM은 신경과학 이론에 기반하여 활성화 에너지가 낮은(정보량이 높은) 뉴런에 더 높은 가중치를 부여함으로써 특징 맵을 정제합니다. 이 두 가지 경량화된 모듈의 통합은 모델의 복잡도를 최소화하면서 성능을 극대화합니다.</p>
<h3 id="5-what-is-achieved-with-the-new-method-달성한-성과">5) What is achieved with the new method? (달성한 성과)<a hidden class="anchor" aria-hidden="true" href="#5-what-is-achieved-with-the-new-method-달성한-성과">#</a></h3>
<p>DAUNet은 두 가지 데이터셋에서 SOTA 성능을 달성했습니다.</p>
<table>
<thead>
<tr>
<th style="text-align:left">데이터셋</th>
<th style="text-align:left">모델</th>
<th style="text-align:left">DSC (↑)</th>
<th style="text-align:left">HD95 (↓)</th>
<th style="text-align:left">ASD (↓)</th>
<th style="text-align:left">Param (M) (↓)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>FH-PS-AoP</strong></td>
<td style="text-align:left"><strong>DAUNet (Proposed)</strong></td>
<td style="text-align:left"><strong>89.09%</strong></td>
<td style="text-align:left"><strong>10.37</strong></td>
<td style="text-align:left"><strong>3.70</strong></td>
<td style="text-align:left"><strong>20.47</strong></td>
</tr>
<tr>
<td style="text-align:left">FH-PS-AoP</td>
<td style="text-align:left">TransUNet</td>
<td style="text-align:left">87.34%</td>
<td style="text-align:left">13.25</td>
<td style="text-align:left">3.67</td>
<td style="text-align:left">105.28</td>
</tr>
<tr>
<td style="text-align:left">FH-PS-AoP</td>
<td style="text-align:left">UNet (Baseline)</td>
<td style="text-align:left">80.22%</td>
<td style="text-align:left">15.87</td>
<td style="text-align:left">4.88</td>
<td style="text-align:left">31.03</td>
</tr>
<tr>
<td style="text-align:left"><strong>FUMPE</strong></td>
<td style="text-align:left"><strong>DAUNet (Proposed)</strong></td>
<td style="text-align:left"><strong>88.80%</strong></td>
<td style="text-align:left"><strong>2.57</strong></td>
<td style="text-align:left"><strong>-</strong></td>
<td style="text-align:left"><strong>20.47</strong></td>
</tr>
<tr>
<td style="text-align:left">FUMPE</td>
<td style="text-align:left">FAT-Net</td>
<td style="text-align:left">84.44%</td>
<td style="text-align:left">3.67</td>
<td style="text-align:left">-</td>
<td style="text-align:left">30.00</td>
</tr>
</tbody>
</table>
<p>FH-PS-AoP 데이터셋에서 DAUNet은 평균 DSC 89.09%를 달성하며, TransUNet (105.28M)보다 5배 이상 적은 매개변수(20.47M)로 더 높은 정확도를 보였습니다. 특히 경계 기반 지표인 HD95와 ASD에서 가장 낮은 수치를 기록하여 우수한 경계 정밀도를 입증했습니다. FUMPE 데이터셋에서도 DSC 88.80%로 최고 성능을 달성했습니다.</p>
<h3 id="6-what-data-are-used-사용-데이터셋">6) What data are used? (사용 데이터셋)<a hidden class="anchor" aria-hidden="true" href="#6-what-data-are-used-사용-데이터셋">#</a></h3>
<p>두 가지 도전적인 의료 영상 분할 데이터셋이 사용되었습니다.</p>
<ul>
<li><strong>FH-PS-AoP (Pubic Symphysis and Fetal Head Detection):</strong> 트랜스페리네알 초음파(transperineal ultrasound) 2D B-모드 영상으로 구성됩니다. 태아 머리와 치골 결합(pubic symphysis) 분할을 목표로 하며, 해부학적 가변성이 크고 저대비 환경이 특징입니다.</li>
<li><strong>FUMPE (Pulmonary Embolism Detection):</strong> CT 혈관조영술(CT angiography, CTA) 3D 스캔 영상으로 구성되며, 폐색전증(PE) 검출을 목표로 합니다. 특히 PE 영역의 약 67%가 말초 폐동맥에 발생하여 정교한 경계 분할이 요구됩니다.</li>
</ul>
<h3 id="7-what-are-the-limitations-저자가-언급한-한계점">7) What are the limitations? (저자가 언급한 한계점)<a hidden class="anchor" aria-hidden="true" href="#7-what-are-the-limitations-저자가-언급한-한계점">#</a></h3>
<p>저자는 DAUNet의 명시적인 한계점을 언급하기보다는, 현재 모델이 2D 영상에 초점을 맞추고 있음을 시사하며 향후 연구 방향을 제시했습니다. 미래 연구는 다음과 같습니다.</p>
<ol>
<li>프레임워크를 멀티모달 및 3D 영상으로 확장.</li>
<li>도메인 적응(domain adaptation)을 통해 교차 도메인 일반화(cross-domain generalization)를 개선.</li>
<li>엣지 디바이스에서의 실시간 추론을 위해 모델을 추가로 최적화.</li>
</ol>
<hr>
<h2 id="3-아키텍처-및-방법론-architecture--methodology">3. 아키텍처 및 방법론 (Architecture &amp; Methodology)<a hidden class="anchor" aria-hidden="true" href="#3-아키텍처-및-방법론-architecture--methodology">#</a></h2>
<h3 id="figure-분석-daunet-아키텍처-figure-3">Figure 분석: DAUNet 아키텍처 (Figure 3)<a hidden class="anchor" aria-hidden="true" href="#figure-분석-daunet-아키텍처-figure-3">#</a></h3>
<p>DAUNet은 고전적인 U-Net의 인코더-디코더 구조를 기반으로 하며, 특히 병목 블록과 스킵 연결 경로에 핵심적인 수정 사항을 도입했습니다.</p>
<p><strong>U-Net 구조에서 변경된 블록 및 흐름:</strong></p>
<ol>
<li><strong>인코더 및 디코더 블록:</strong> 기존 U-Net과 유사하게 다운스케일링(Downscaling) 및 업스케일링(Upscaling) 경로를 가지며, 각 레벨은 <code>DoubleConv (DC)</code> 블록을 사용합니다. 모든 컨볼루션 연산은 $3 \times 3$ 필터를 사용합니다.</li>
<li><strong>병목 (Bottleneck) 블록의 재설계:</strong>
<ul>
<li>기존 U-Net의 일반 컨볼루션 대신, DAUNet의 병목은 네 단계의 연속적인 연산으로 구성된 복합 구조를 사용합니다.</li>
<li><strong>채널 압축:</strong> $1 \times 1$ 컨볼루션을 사용하여 입력 채널을 목표 출력 채널의 1/4로 압축하여 계산 비용을 제어합니다.</li>
<li><strong>공간 적응:</strong> $3 \times 3$ <strong>Deformable Convolution V2 (DCN V2)</strong> 레이어를 적용하여 동적 오프셋을 학습하고 입력 특징의 기하학적 변형을 포착합니다.</li>
<li><strong>채널 복원:</strong> 두 번째 $1 \times 1$ 컨볼루션을 사용하여 특징을 원래 채널 차원으로 다시 투영합니다.</li>
<li><strong>특징 정제:</strong> <strong>SimAM</strong> 모듈을 마지막에 추가하여 에너지 기반 기준으로 공간적으로 정보가 풍부한 활성화를 강조하고 출력을 정제합니다.</li>
</ul>
</li>
<li><strong>스킵 연결 (Skip Connections) 강화:</strong>
<ul>
<li>인코더와 디코더 특징을 병합(Merge)하기 전에, 모든 스킵 연결 경로에 <strong>SimAM</strong> 모듈이 통합됩니다.</li>
<li>이 SimAM 모듈은 인코더에서 디코더로 전달되는 특징 맵의 관련성이 낮은 활성화를 억제하고 의미적으로 풍부한 특징의 전송을 강화합니다.</li>
</ul>
</li>
</ol>
<h3 id="수식-상세">수식 상세<a hidden class="anchor" aria-hidden="true" href="#수식-상세">#</a></h3>
<h4 id="1-modulated-deformable-convolution-v2-변조된-변형-가능-컨볼루션-v2">1. Modulated Deformable Convolution V2 (변조된 변형 가능 컨볼루션 V2)<a hidden class="anchor" aria-hidden="true" href="#1-modulated-deformable-convolution-v2-변조된-변형-가능-컨볼루션-v2">#</a></h4>
<p>DCN V2는 표준 컨볼루션의 고정된 샘플링 위치($k$)를 학습 가능한 오프셋($\Delta p_k$)으로 조정하고, 변조 스칼라($\alpha_k$)를 도입하여 특정 영역을 선택적으로 강조하거나 억제합니다. 출력 $Y$는 위치 $p$에서 다음과 같이 계산됩니다.</p>
<p>$$Y(p) = \sum_{k \in \mathcal{R}} \alpha_k \cdot K(k) \cdot F(p + k + \Delta p_k)$$</p>
<ul>
<li>$F$: 입력 특징 맵</li>
<li>$K$: 컨볼루션 커널</li>
<li>$\mathcal{R}$: 수용장(receptive field)</li>
<li>$\Delta p_k$: 학습 가능한 오프셋 (입력 특징에 따라 동적으로 조정됨)</li>
<li>$\alpha_k$: 변조 스칼라 ($[0, 1]$ 범위, 특정 영역의 중요도를 조절)</li>
</ul>
<h4 id="2-parameter-free-attention-simam-simple-attention-module">2. Parameter-Free Attention: SimAM (Simple Attention Module)<a hidden class="anchor" aria-hidden="true" href="#2-parameter-free-attention-simam-simple-attention-module">#</a></h4>
<p>SimAM은 신경과학 이론에 영감을 받아, 활성화 에너지가 높은 뉴런이 덜 유익하다는 가정 하에 각 뉴런의 중요도를 에너지 기반 함수를 통해 평가합니다.</p>
<p><strong>에너지 함수 ($E_t$) (Eq. 3):</strong></p>
<p>주어진 특징 맵 $X \in \mathbb{R}^{C \times H \times W}$에서, 위치 $t$의 뉴런에 대한 에너지 $E_t$는 다음과 같습니다.</p>
<p>$$E_t = (x_t - \mu_t)^2 + \frac{1}{\lambda} \sum_{i \neq t} (x_i - \mu_t)^2$$</p>
<ul>
<li>$x_t$: 대상 뉴런의 활성화 값</li>
<li>$\mu_t$: $x_t$를 제외한 동일 채널 내 모든 뉴런의 평균</li>
<li>$\lambda$: 주변 뉴런의 중요도를 제어하는 하이퍼파라미터 ($1 \times 10^{-4}$로 설정됨)</li>
</ul>
<p><strong>어텐션 가중치 ($a_t$) (Eq. 4):</strong></p>
<p>각 뉴런에 대한 어텐션 가중치 $a_t$는 에너지 함수를 시그모이드 활성화 함수 $\sigma(\cdot)$를 통해 계산됩니다.</p>
<p>$$a_t = \frac{1}{\sigma(\frac{E_t}{\epsilon}) + \epsilon}$$</p>
<ul>
<li>$\epsilon$: 0으로 나누는 것을 방지하는 작은 상수</li>
</ul>
<p><strong>정제된 출력 ($X&rsquo;$) (Eq. 5):</strong></p>
<p>정제된 출력 특징 맵 $X&rsquo;$는 원래 특징 맵 $X$와 어텐션 맵 $A$의 요소별 곱셈으로 얻어집니다.</p>
<p>$$X&rsquo; = X \odot A$$</p>
<h4 id="3-loss-function-손실-함수">3. Loss Function (손실 함수)<a hidden class="anchor" aria-hidden="true" href="#3-loss-function-손실-함수">#</a></h4>
<p>학습 과정에서는 <strong>하이브리드 손실 함수</strong>가 사용되었습니다.</p>
<p>$$\text{Loss} = \text{Dice Loss} + \text{Weighted Binary Cross-Entropy (BCE)}$$</p>
<p>이 조합은 분할 정확도(Dice Loss)와 클래스 불균형 문제 처리(Weighted BCE)를 동시에 목표로 합니다.</p>
<h3 id="vanilla-u-net-비교">Vanilla U-Net 비교<a hidden class="anchor" aria-hidden="true" href="#vanilla-u-net-비교">#</a></h3>
<table>
<thead>
<tr>
<th style="text-align:left">특징</th>
<th style="text-align:left">Vanilla U-Net</th>
<th style="text-align:left">DAUNet (Proposed)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>기본 구조</strong></td>
<td style="text-align:left">인코더-디코더, 스킵 연결</td>
<td style="text-align:left">인코더-디코더, 스킵 연결</td>
</tr>
<tr>
<td style="text-align:left"><strong>컨볼루션 유형</strong></td>
<td style="text-align:left">고정 그리드 컨볼루션</td>
<td style="text-align:left">DCN V2 (병목), 표준 컨볼루션 (인코더/디코더)</td>
</tr>
<tr>
<td style="text-align:left"><strong>병목 블록</strong></td>
<td style="text-align:left">일반 컨볼루션 레이어</td>
<td style="text-align:left">$1 \times 1$ Conv $\rightarrow$ DCN V2 $\rightarrow$ $1 \times 1$ Conv $\rightarrow$ SimAM</td>
</tr>
<tr>
<td style="text-align:left"><strong>스킵 연결</strong></td>
<td style="text-align:left">인코더 특징을 디코더에 직접 연결 (Concatenation)</td>
<td style="text-align:left">연결 전에 <strong>SimAM 모듈</strong>을 적용하여 특징 정제 후 연결</td>
</tr>
<tr>
<td style="text-align:left"><strong>어텐션 메커니즘</strong></td>
<td style="text-align:left">없음 (Attention U-Net 제외)</td>
<td style="text-align:left">SimAM (Parameter-Free Attention) 사용</td>
</tr>
<tr>
<td style="text-align:left"><strong>매개변수 수 (FH-PS-AoP)</strong></td>
<td style="text-align:left">31.03M</td>
<td style="text-align:left"><strong>20.47M</strong></td>
</tr>
</tbody>
</table>
<hr>
<h2 id="4-태그-제안-tags-suggestion">4. 태그 제안 (Tags Suggestion)<a hidden class="anchor" aria-hidden="true" href="#4-태그-제안-tags-suggestion">#</a></h2>
<ol>
<li>Deformable Convolutions (변형 가능 컨볼루션)</li>
<li>Parameter-Free Attention (매개변수 없는 어텐션)</li>
<li>Lightweight UNet (경량 U-Net)</li>
<li>Medical Image Segmentation (의료 영상 분할)</li>
<li>Real-Time Deployment (실시간 배포)</li>
</ol>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://hanwo-ol.github.io/Unet_master/tags/deformable-convolutions-%EB%B3%80%ED%98%95-%EA%B0%80%EB%8A%A5-%EC%BB%A8%EB%B3%BC%EB%A3%A8%EC%85%98/">Deformable Convolutions (변형 가능 컨볼루션)</a></li>
      <li><a href="https://hanwo-ol.github.io/Unet_master/tags/parameter-free-attention-%EB%A7%A4%EA%B0%9C%EB%B3%80%EC%88%98-%EC%97%86%EB%8A%94-%EC%96%B4%ED%85%90%EC%85%98/">Parameter-Free Attention (매개변수 없는 어텐션)</a></li>
      <li><a href="https://hanwo-ol.github.io/Unet_master/tags/lightweight-unet-%EA%B2%BD%EB%9F%89-u-net/">Lightweight UNet (경량 U-Net)</a></li>
      <li><a href="https://hanwo-ol.github.io/Unet_master/tags/medical-image-segmentation-%EC%9D%98%EB%A3%8C-%EC%98%81%EC%83%81-%EB%B6%84%ED%95%A0/">Medical Image Segmentation (의료 영상 분할)</a></li>
      <li><a href="https://hanwo-ol.github.io/Unet_master/tags/real-time-deployment-%EC%8B%A4%EC%8B%9C%EA%B0%84-%EB%B0%B0%ED%8F%AC/">Real-Time Deployment (실시간 배포)</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://hanwo-ol.github.io/Unet_master/posts/robust-variational-model-based-tailored-unet-leveraging-edge-detector-and-mean-curvature-for-improved-image-segmentation/">
    <span class="title">« Prev</span>
    <br>
    <span>Robust Variational Model Based Tailored UNet: Leveraging Edge Detector and Mean Curvature for Improved Image Segmentation</span>
  </a>
  <a class="next" href="https://hanwo-ol.github.io/Unet_master/posts/lean-unet-a-compact-model-for-image-segmentation/">
    <span class="title">Next »</span>
    <br>
    <span>Lean Unet: A Compact Model for Image Segmentation</span>
  </a>
</nav>


<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share DAUNet: A Lightweight UNet Variant with Deformable Convolutions and Parameter-Free Attention for Medical Image Segmentation on twitter"
        href="https://twitter.com/intent/tweet/?text=DAUNet%3a%20A%20Lightweight%20UNet%20Variant%20with%20Deformable%20Convolutions%20and%20Parameter-Free%20Attention%20for%20Medical%20Image%20Segmentation&amp;url=https%3a%2f%2fhanwo-ol.github.io%2fUnet_master%2fposts%2fdaunet-a-lightweight-unet-variant-with-deformable-convolutions-and-parameter-free-attention-for-medical-image-segmentation%2f&amp;hashtags=DeformableConvolutions%28%eb%b3%80%ed%98%95%ea%b0%80%eb%8a%a5%ec%bb%a8%eb%b3%bc%eb%a3%a8%ec%85%98%29%2cParameter-FreeAttention%28%eb%a7%a4%ea%b0%9c%eb%b3%80%ec%88%98%ec%97%86%eb%8a%94%ec%96%b4%ed%85%90%ec%85%98%29%2cLightweightUNet%28%ea%b2%bd%eb%9f%89U-Net%29%2cMedicalImageSegmentation%28%ec%9d%98%eb%a3%8c%ec%98%81%ec%83%81%eb%b6%84%ed%95%a0%29%2cReal-TimeDeployment%28%ec%8b%a4%ec%8b%9c%ea%b0%84%eb%b0%b0%ed%8f%ac%29">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share DAUNet: A Lightweight UNet Variant with Deformable Convolutions and Parameter-Free Attention for Medical Image Segmentation on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fhanwo-ol.github.io%2fUnet_master%2fposts%2fdaunet-a-lightweight-unet-variant-with-deformable-convolutions-and-parameter-free-attention-for-medical-image-segmentation%2f&amp;title=DAUNet%3a%20A%20Lightweight%20UNet%20Variant%20with%20Deformable%20Convolutions%20and%20Parameter-Free%20Attention%20for%20Medical%20Image%20Segmentation&amp;summary=DAUNet%3a%20A%20Lightweight%20UNet%20Variant%20with%20Deformable%20Convolutions%20and%20Parameter-Free%20Attention%20for%20Medical%20Image%20Segmentation&amp;source=https%3a%2f%2fhanwo-ol.github.io%2fUnet_master%2fposts%2fdaunet-a-lightweight-unet-variant-with-deformable-convolutions-and-parameter-free-attention-for-medical-image-segmentation%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share DAUNet: A Lightweight UNet Variant with Deformable Convolutions and Parameter-Free Attention for Medical Image Segmentation on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2fhanwo-ol.github.io%2fUnet_master%2fposts%2fdaunet-a-lightweight-unet-variant-with-deformable-convolutions-and-parameter-free-attention-for-medical-image-segmentation%2f&title=DAUNet%3a%20A%20Lightweight%20UNet%20Variant%20with%20Deformable%20Convolutions%20and%20Parameter-Free%20Attention%20for%20Medical%20Image%20Segmentation">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share DAUNet: A Lightweight UNet Variant with Deformable Convolutions and Parameter-Free Attention for Medical Image Segmentation on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fhanwo-ol.github.io%2fUnet_master%2fposts%2fdaunet-a-lightweight-unet-variant-with-deformable-convolutions-and-parameter-free-attention-for-medical-image-segmentation%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share DAUNet: A Lightweight UNet Variant with Deformable Convolutions and Parameter-Free Attention for Medical Image Segmentation on whatsapp"
        href="https://api.whatsapp.com/send?text=DAUNet%3a%20A%20Lightweight%20UNet%20Variant%20with%20Deformable%20Convolutions%20and%20Parameter-Free%20Attention%20for%20Medical%20Image%20Segmentation%20-%20https%3a%2f%2fhanwo-ol.github.io%2fUnet_master%2fposts%2fdaunet-a-lightweight-unet-variant-with-deformable-convolutions-and-parameter-free-attention-for-medical-image-segmentation%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share DAUNet: A Lightweight UNet Variant with Deformable Convolutions and Parameter-Free Attention for Medical Image Segmentation on telegram"
        href="https://telegram.me/share/url?text=DAUNet%3a%20A%20Lightweight%20UNet%20Variant%20with%20Deformable%20Convolutions%20and%20Parameter-Free%20Attention%20for%20Medical%20Image%20Segmentation&amp;url=https%3a%2f%2fhanwo-ol.github.io%2fUnet_master%2fposts%2fdaunet-a-lightweight-unet-variant-with-deformable-convolutions-and-parameter-free-attention-for-medical-image-segmentation%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://hanwo-ol.github.io/Unet_master/">Research Agent Knowledge Base</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
