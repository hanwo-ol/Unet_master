<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Satellite to Street : Disaster Impact Estimator | Research Agent Knowledge Base</title>
<meta name="keywords" content="**Disaster Assessment (재난 평가)**, **Bitemporal Analysis (이시점 분석)**, **Semantic Segmentation (의미론적 분할)**, **SE-ResNeXt (SE-ResNeXt)**, **Damage Classification (피해 분류)**">
<meta name="description" content="Abstract Accurate post-disaster damage assessment is of high importance for prioritizing emergency response; however, manual interpretation of satellite imagery is slow, subjective, and hard to scale. While deep-learning models for image segmentation, such as U-Net-based baselines and change-detection models, are useful baselines, they often struggle with subtle structural variations and severe class imbalance, yielding poor detection of highly damaged regions. The present work proposes a deep-learning framework that jointly processes pre- and post-disaster satellite images to obtain fine-grained pixel-level damage maps: Satellite-to-Street: Disaster Impact Estimator.">
<meta name="author" content="">
<link rel="canonical" href="https://hanwo-ol.github.io/Unet_master/posts/satellite-to-street-disaster-impact-estimator/">
<link crossorigin="anonymous" href="/Unet_master/assets/css/stylesheet.62aa25427797f8efd87301a5b69795dc50df2dbe79a5fba0648cc7bb8dbcd7c9.css" integrity="sha256-YqolQneX&#43;O/YcwGltpeV3FDfLb55pfugZIzHu42818k=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/Unet_master/assets/js/highlight.acb54fd32bbc1982428b8850317e45d076b95012730a5936667e6bc21777692a.js" integrity="sha256-rLVP0yu8GYJCi4hQMX5F0Ha5UBJzClk2Zn5rwhd3aSo="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://hanwo-ol.github.io/Unet_master/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://hanwo-ol.github.io/Unet_master/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://hanwo-ol.github.io/Unet_master/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://hanwo-ol.github.io/Unet_master/apple-touch-icon.png">
<link rel="mask-icon" href="https://hanwo-ol.github.io/Unet_master/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"
    integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"
    integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8"
    crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05"
    crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function () {
        renderMathInElement(document.body, {
            delimiters: [
                { left: "$$", right: "$$", display: true },
                { left: "$", right: "$", display: false },
                { left: "\\(", right: "\\)", display: false },
                { left: "\\[", right: "\\]", display: true }
            ]
        });
    });
</script>
<meta property="og:title" content="Satellite to Street : Disaster Impact Estimator" />
<meta property="og:description" content="Abstract Accurate post-disaster damage assessment is of high importance for prioritizing emergency response; however, manual interpretation of satellite imagery is slow, subjective, and hard to scale. While deep-learning models for image segmentation, such as U-Net-based baselines and change-detection models, are useful baselines, they often struggle with subtle structural variations and severe class imbalance, yielding poor detection of highly damaged regions. The present work proposes a deep-learning framework that jointly processes pre- and post-disaster satellite images to obtain fine-grained pixel-level damage maps: Satellite-to-Street: Disaster Impact Estimator." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://hanwo-ol.github.io/Unet_master/posts/satellite-to-street-disaster-impact-estimator/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2025-11-24T00:00:00+00:00" />
<meta property="article:modified_time" content="2025-11-24T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Satellite to Street : Disaster Impact Estimator"/>
<meta name="twitter:description" content="Abstract Accurate post-disaster damage assessment is of high importance for prioritizing emergency response; however, manual interpretation of satellite imagery is slow, subjective, and hard to scale. While deep-learning models for image segmentation, such as U-Net-based baselines and change-detection models, are useful baselines, they often struggle with subtle structural variations and severe class imbalance, yielding poor detection of highly damaged regions. The present work proposes a deep-learning framework that jointly processes pre- and post-disaster satellite images to obtain fine-grained pixel-level damage maps: Satellite-to-Street: Disaster Impact Estimator."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://hanwo-ol.github.io/Unet_master/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Satellite to Street : Disaster Impact Estimator",
      "item": "https://hanwo-ol.github.io/Unet_master/posts/satellite-to-street-disaster-impact-estimator/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Satellite to Street : Disaster Impact Estimator",
  "name": "Satellite to Street : Disaster Impact Estimator",
  "description": "Abstract Accurate post-disaster damage assessment is of high importance for prioritizing emergency response; however, manual interpretation of satellite imagery is slow, subjective, and hard to scale. While deep-learning models for image segmentation, such as U-Net-based baselines and change-detection models, are useful baselines, they often struggle with subtle structural variations and severe class imbalance, yielding poor detection of highly damaged regions. The present work proposes a deep-learning framework that jointly processes pre- and post-disaster satellite images to obtain fine-grained pixel-level damage maps: Satellite-to-Street: Disaster Impact Estimator.",
  "keywords": [
    "**Disaster Assessment (재난 평가)**", "**Bitemporal Analysis (이시점 분석)**", "**Semantic Segmentation (의미론적 분할)**", "**SE-ResNeXt (SE-ResNeXt)**", "**Damage Classification (피해 분류)**"
  ],
  "articleBody": "Abstract Accurate post-disaster damage assessment is of high importance for prioritizing emergency response; however, manual interpretation of satellite imagery is slow, subjective, and hard to scale. While deep-learning models for image segmentation, such as U-Net-based baselines and change-detection models, are useful baselines, they often struggle with subtle structural variations and severe class imbalance, yielding poor detection of highly damaged regions. The present work proposes a deep-learning framework that jointly processes pre- and post-disaster satellite images to obtain fine-grained pixel-level damage maps: Satellite-to-Street: Disaster Impact Estimator. The model uses a modified dual-input U-Net architecture with enhanced feature fusion to capture both the local structural changes as well as the broader contextual cues. Class-aware weighted loss functions are integrated in order to handle the dominance of undamaged pixels in real disaster datasets, thus enhancing sensitivity toward major and destroyed categories. Experimentation on publicly available disaster datasets shows improved localization and classification of structural damage when compared to traditional segmentation and baseline change-detection models. The resulting damage maps provide a rapid and consistent assessment mechanism to support and not replace expert decision-making, thus allowing more efficient, data-driven disaster management.\nPDF Download Local PDF View | Arxiv Original\nResearch Agent - Draft Refiner Module 리포트 1. 요약 (Executive Summary) 본 논문은 위성 이미지를 활용하여 재난 후 피해 정도를 신속하고 정확하게 추정하는 딥러닝 프레임워크인 **“Satellite-to-Street: Disaster Impact Estimator”**를 제안합니다.\n문제 정의: 기존의 수동적인 재난 피해 평가는 느리고 주관적이며, 딥러닝 기반 모델은 미묘한 구조적 변화를 감지하는 데 어려움을 겪고, 특히 ‘피해 없음’ 픽셀의 압도적인 우세로 인해 심각한 클래스 불균형 문제를 겪습니다. 핵심 방법론: 재난 발생 전후의 위성 이미지를 공동으로 처리하는 수정된 이중 입력(Dual-Input) U-Net 아키텍처를 사용합니다. 아키텍처 개선: 인코더로 SE-ResNeXt50을 통합하여 계층적 특징 추출을 강화하고, 채널별 주의(Channel-wise attention) 메커니즘을 통해 미세한 구조적 변화를 더 잘 포착합니다. 손실 함수: 실제 데이터셋에서 흔히 발생하는 클래스 불균형을 해결하기 위해 **클래스 가중치 손실 함수(Class-aware weighted loss)**를 적용하여 ‘주요 피해’ 및 ‘파괴’ 카테고리에 대한 민감도를 높였습니다. 결과 및 활용: 픽셀 수준의 피해 분류 마스크를 생성하며, 이를 도로 네트워크 정보와 결합하여 거리 수준(Street-Level) 영향 분석을 수행함으로써 응급 대응을 위한 실행 가능한 우선순위 히트맵을 제공합니다. 2. 7가지 핵심 질문 분석 (Key Analysis) 1) What is new in the work? (기존 연구와의 차별점) 본 연구는 재난 전후 위성 이미지를 공동으로 처리하여 픽셀 수준의 정밀한 피해 지도를 얻는 딥러닝 프레임워크를 제안합니다. 가장 큰 차별점은 SE-ResNeXt50 인코더를 통합한 수정된 이중 입력 U-Net 아키텍처를 사용하여 미묘한 구조적 변화를 효과적으로 식별한다는 점입니다. 또한, 픽셀 수준의 예측 결과를 OpenStreetMap 지오메트리와 결합하여 **거리 수준의 영향 분석(Street-Level Impact Analysis)**을 수행하는 후처리 모듈을 도입하여, 단순한 피해 감지를 넘어 재난 대응을 위한 운영 계획(Operational Planning)에 직접 활용 가능한 정보를 제공합니다.\n2) Why is the work important? (연구의 중요성) 이 연구는 재난 발생 직후 응급 대응 우선순위를 정하는 데 필수적인 피해 평가 과정을 자동화하고 가속화합니다. 기존의 수동 평가 방식은 시간이 오래 걸리고 오류 발생 가능성이 높았으나, 이 시스템은 신속하고 일관된 평가 메커니즘을 제공합니다. 특히, 거리 수준의 영향 분석을 통해 피해가 집중된 주요 경로를 식별하는 우선순위 히트맵을 생성함으로써, 구호 기관이 자원을 효율적으로 배분하고 데이터 기반의 재난 관리를 수행할 수 있도록 지원합니다.\n3) What is the literature gap? (기존 연구의 한계점) 기존의 U-Net 기반 세그멘테이션 및 변화 감지 모델들은 두 가지 주요 한계에 직면했습니다. 첫째, 미묘한 구조적 변화를 정확하게 포착하는 데 어려움을 겪어 정밀한 피해 분류가 어려웠습니다. 둘째, 실제 재난 데이터셋에서 ‘피해 없음’ 픽셀이 압도적으로 많아 발생하는 **심각한 클래스 불균형(Severe Class Imbalance)**으로 인해, 모델이 ‘주요 피해’나 ‘파괴’와 같은 희귀한 피해 카테고리를 제대로 감지하지 못했습니다.\n4) How is the gap filled? (해결 방안) 이러한 한계는 세 가지 방식으로 해결되었습니다. 첫째, 이중 입력(Bitemporal) 방식을 채택하여 재난 전후 이미지를 6채널 입력으로 결합함으로써 구조적 변화를 명확히 학습합니다. 둘째, 인코더로 SE-ResNeXt50을 사용하여 채널별 주의 메커니즘(Squeeze-and-Excitation)과 그룹 컨볼루션을 통해 계층적 특징 추출 능력을 높이고 미세한 변화를 식별합니다. 셋째, **클래스 가중치 교차 엔트로피 손실 함수(Class-weighted cross-entropy loss)**를 적용하여 희귀한 피해 클래스에 더 높은 가중치를 부여함으로써 클래스 불균형 문제를 완화하고 주요 피해 감지 성능을 향상시켰습니다.\n5) What is achieved with the new method? (달성한 성과) xBD 벤치마크 데이터셋에 대한 실험 결과, 제안된 SE-ResNeXt50 U-Net 모델은 표준 ResNet-50 U-Net 모델 대비 우수한 성능을 달성했습니다 (Table 6.2.1).\nEncoder mIoU Dice ResNet-50 U-Net 0.69 0.76 SE-ResNeXt50 U-Net 0.74 0.81 SE-ResNeXt50 U-Net은 평균 IoU(mIoU)에서 0.74를, Dice 점수에서 0.81을 기록하며, 표준 모델 대비 성능이 향상되었음을 입증했습니다. 특히, Table 6.1.1에 따르면 가장 중요한 ‘Destroyed’ 클래스에서 IoU 0.75, Dice 0.83의 높은 성능을 보여, 심각한 피해를 정확하게 분류하는 능력을 확인했습니다.\n6) What data are used? (사용 데이터셋) 주요 학습 및 테스트에는 대규모 재난 매핑 데이터셋인 xBD와 xView2가 사용되었습니다. 이 데이터셋들은 고해상도 위성 이미지 타일과 포괄적인 폴리곤 주석을 포함하며, 지진, 홍수, 허리케인, 산불 등 광범위한 재난 유형과 다중 클래스 피해 분류를 제공합니다. 건물 수준 주석을 보완하기 위해 SpaceNet, OpenStreetMap, Open Buildings project와 같은 지리공간 소스도 활용되어 도로 및 정착지 레이아웃 정보를 제공했습니다.\n7) What are the limitations? (저자가 언급한 한계점) 저자들은 다음과 같은 한계점과 향후 연구 방향을 제시했습니다.\n데이터 의존성: 현재 모델은 광학 데이터에 의존하므로 구름, 연기, 야간 상황 등 광학 데이터가 부족한 상황에서는 신뢰도가 떨어집니다. (향후 SAR 또는 다중 스펙트럼 이미지 통합 필요) 장거리 구조적 의존성 포착: 밀집 지역에서 장거리 구조적 의존성을 포착하는 능력을 향상시키기 위해 Transformer 기반 인코더 또는 하이브리드 CNN-ViT 설계가 필요합니다. 거리 수준 분석의 정교화: 피해가 상호 연결된 도로 네트워크를 통해 어떻게 확산되는지 더 정확하게 모델링하기 위해 거리 수준 분석에 **그래프 신경망(GNNs)**을 통합해야 합니다. 일반화: 모델의 일반화 능력을 높이기 위해 더 다양한 재난 시나리오를 포함하는 데이터셋 확장이 필요합니다. 3. 아키텍처 및 방법론 (Architecture \u0026 Methodology) Figure 분석: 메인 아키텍처 본 논문에서 제안하는 시스템은 이중 입력(Bitemporal) U-Net을 기반으로 하며, 도시 환경(Dense Topographic Features)과 농촌 환경(Sparse Topographic Features) 모두에 적용 가능하도록 설계되었습니다 (Figure 1).\n핵심 아키텍처 (Figure 5.4.1): SE-ResNeXt50 U-Net\n입력: 재난 전 이미지와 재난 후 이미지를 결합하여 6채널 입력 텐서로 사용합니다. 인코더: U-Net의 인코더 부분은 SE-ResNeXt50 모듈을 사용합니다. 이 인코더는 깊은 주의 모듈(Deep attention modules)과 가중치 공유(Share weight)를 통해 재난 전후 이미지의 특징을 비교하고 구조적 변화를 추출합니다. 디코더 및 스킵 연결: 인코더에서 추출된 계층적 특징은 디코더로 전달되며, U-Net의 핵심인 **스킵 연결(Skip connection)**을 통해 인코더의 저수준 공간 정보가 디코더로 직접 전달되어 정밀한 픽셀 수준의 세그멘테이션을 가능하게 합니다. 출력: 픽셀 수준의 7가지 클래스 피해 마스크(No Damage, Minor Damage, Major Damage, Destroyed 등)를 출력합니다. 후처리 (Street-Level Impact Analysis): 픽셀 수준의 예측 결과를 OpenStreetMap의 도로 세그먼트와 연결하여 거리별 피해 점수($S_j$)를 계산하고 우선순위 히트맵을 생성합니다 (Figure 2 참조). 수식 상세 1. 입력/출력 텐서 형태 (Input/Output Tensor Shape) 입력 텐서 ($I_{input}$): 재난 전 이미지(RGB, 3채널)와 재난 후 이미지(RGB, 3채널)를 채널 축으로 쌓아 6채널로 구성합니다. $$I_{input} \\in \\mathbb{R}^{H \\times W \\times 6}$$ 여기서 $H=256, W=256$ (실험에서 사용된 해상도).\n출력 텐서 ($M_{output}$): 픽셀별 7가지 피해 클래스에 대한 확률 마스크입니다. $$M_{output} \\in \\mathbb{R}^{H \\times W \\times 7}$$\n2. 손실 함수 (Loss Function) 모델 학습에는 심각한 클래스 불균형(특히 ‘피해 없음’ 픽셀의 우세)을 해결하기 위해 **클래스 가중치 교차 엔트로피 손실(Class-Weighted Cross-Entropy Loss)**이 사용되었습니다.\n$$L = - \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{c=1}^{C} w_c \\cdot y_{i,c} \\log(\\hat{y}_{i,c})$$\n$N$: 전체 픽셀 수 $C$: 피해 클래스 수 (7개) $w_c$: 클래스 $c$에 할당된 가중치 (희귀한 피해 클래스에 더 높은 가중치 부여) $y_{i,c}$: 픽셀 $i$의 클래스 $c$에 대한 실제 레이블 (원-핫 인코딩) $\\hat{y}_{i,c}$: 픽셀 $i$의 클래스 $c$에 대한 예측 확률 3. 거리 피해 지수 (Street Damage Index, $S_j$) 픽셀 수준의 피해 예측을 거리 수준의 영향으로 변환하기 위해 정규화된 가중치 공식이 사용됩니다 (Section 5.4).\n$$S_j = \\frac{1}{N_j} \\sum_{i \\in B_j} w_i \\cdot d_i$$\n$S_j$: 거리 $j$의 피해 점수 (Damage score of the street) $B_j$: 거리 $j$를 따라 위치한 건물들의 집합 (Set of buildings along street $j$) $d_i$: 건물 $i$의 피해 수준 (Damage level of building $i$, 1~4 등급) $w_i$: 건물 면적에 비례하는 가중치 (Weight proportional to building area) $N_j$: $B_j$에 속한 건물 수 Vanilla U-Net 비교 특징 Vanilla U-Net (Ronneberger et al., 2015) Satellite-to-Street (본 연구) 입력 단일 이미지 (3채널) 이중 입력 (Bitemporal): 재난 전후 이미지 스택 (6채널) 인코더 표준 Convolutional Block SE-ResNeXt50 인코더 핵심 모듈 추가/수정 없음 Squeeze-and-Excitation (SE) 모듈 및 그룹 컨볼루션을 통한 채널별 주의 메커니즘 추가 목표 생의학 이미지 세그멘테이션 이시점 변화 감지 및 7클래스 피해 분류 손실 함수 일반적인 교차 엔트로피 클래스 가중치 교차 엔트로피 손실 (클래스 불균형 해결) 후처리 없음 거리 수준 영향 분석 (OpenStreetMap 기반) 4. 태그 제안 (Tags Suggestion) Disaster Assessment (재난 평가) Bitemporal Analysis (이시점 분석) Semantic Segmentation (의미론적 분할) SE-ResNeXt (SE-ResNeXt) Damage Classification (피해 분류) ",
  "wordCount" : "1324",
  "inLanguage": "en",
  "datePublished": "2025-11-24T00:00:00Z",
  "dateModified": "2025-11-24T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://hanwo-ol.github.io/Unet_master/posts/satellite-to-street-disaster-impact-estimator/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Research Agent Knowledge Base",
    "logo": {
      "@type": "ImageObject",
      "url": "https://hanwo-ol.github.io/Unet_master/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://hanwo-ol.github.io/Unet_master/" accesskey="h" title="Research Agent Knowledge Base (Alt + H)">Research Agent Knowledge Base</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://hanwo-ol.github.io/Unet_master/categories/" title="categories">
                    <span>categories</span>
                </a>
            </li>
            <li>
                <a href="https://hanwo-ol.github.io/Unet_master/tags/" title="tags">
                    <span>tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://hanwo-ol.github.io/Unet_master/">Home</a>&nbsp;»&nbsp;<a href="https://hanwo-ol.github.io/Unet_master/posts/">Posts</a></div>
    <h1 class="post-title">
      Satellite to Street : Disaster Impact Estimator
    </h1>
    <div class="post-meta">&lt;span title=&#39;2025-11-24 00:00:00 &#43;0000 UTC&#39;&gt;November 24, 2025&lt;/span&gt;&amp;nbsp;·&amp;nbsp;7 min

</div>
  </header> 
  <div class="post-content"><h2 id="abstract">Abstract<a hidden class="anchor" aria-hidden="true" href="#abstract">#</a></h2>
<p>Accurate post-disaster damage assessment is of high importance for prioritizing emergency response; however, manual interpretation of satellite imagery is slow, subjective, and hard to scale. While deep-learning models for image segmentation, such as U-Net-based baselines and change-detection models, are useful baselines, they often struggle with subtle structural variations and severe class imbalance, yielding poor detection of highly damaged regions. The present work proposes a deep-learning framework that jointly processes pre- and post-disaster satellite images to obtain fine-grained pixel-level damage maps: Satellite-to-Street: Disaster Impact Estimator. The model uses a modified dual-input U-Net architecture with enhanced feature fusion to capture both the local structural changes as well as the broader contextual cues. Class-aware weighted loss functions are integrated in order to handle the dominance of undamaged pixels in real disaster datasets, thus enhancing sensitivity toward major and destroyed categories. Experimentation on publicly available disaster datasets shows improved localization and classification of structural damage when compared to traditional segmentation and baseline change-detection models. The resulting damage maps provide a rapid and consistent assessment mechanism to support and not replace expert decision-making, thus allowing more efficient, data-driven disaster management.</p>
<h2 id="pdf-download">PDF Download<a hidden class="anchor" aria-hidden="true" href="#pdf-download">#</a></h2>
<p><a href="//172.22.138.185/Research_pdf/2512.00065v1.pdf">Local PDF View</a> | <a href="http://arxiv.org/abs/2512.00065v1">Arxiv Original</a></p>
<h2 id="research-agent---draft-refiner-module-리포트">Research Agent - Draft Refiner Module 리포트<a hidden class="anchor" aria-hidden="true" href="#research-agent---draft-refiner-module-리포트">#</a></h2>
<hr>
<h2 id="1-요약-executive-summary">1. 요약 (Executive Summary)<a hidden class="anchor" aria-hidden="true" href="#1-요약-executive-summary">#</a></h2>
<p>본 논문은 위성 이미지를 활용하여 재난 후 피해 정도를 신속하고 정확하게 추정하는 딥러닝 프레임워크인 **&ldquo;Satellite-to-Street: Disaster Impact Estimator&rdquo;**를 제안합니다.</p>
<ul>
<li><strong>문제 정의:</strong> 기존의 수동적인 재난 피해 평가는 느리고 주관적이며, 딥러닝 기반 모델은 미묘한 구조적 변화를 감지하는 데 어려움을 겪고, 특히 &lsquo;피해 없음&rsquo; 픽셀의 압도적인 우세로 인해 심각한 클래스 불균형 문제를 겪습니다.</li>
<li><strong>핵심 방법론:</strong> 재난 발생 전후의 위성 이미지를 공동으로 처리하는 수정된 <strong>이중 입력(Dual-Input) U-Net 아키텍처</strong>를 사용합니다.</li>
<li><strong>아키텍처 개선:</strong> 인코더로 <strong>SE-ResNeXt50</strong>을 통합하여 계층적 특징 추출을 강화하고, 채널별 주의(Channel-wise attention) 메커니즘을 통해 미세한 구조적 변화를 더 잘 포착합니다.</li>
<li><strong>손실 함수:</strong> 실제 데이터셋에서 흔히 발생하는 클래스 불균형을 해결하기 위해 **클래스 가중치 손실 함수(Class-aware weighted loss)**를 적용하여 &lsquo;주요 피해&rsquo; 및 &lsquo;파괴&rsquo; 카테고리에 대한 민감도를 높였습니다.</li>
<li><strong>결과 및 활용:</strong> 픽셀 수준의 피해 분류 마스크를 생성하며, 이를 도로 네트워크 정보와 결합하여 <strong>거리 수준(Street-Level) 영향 분석</strong>을 수행함으로써 응급 대응을 위한 실행 가능한 우선순위 히트맵을 제공합니다.</li>
</ul>
<hr>
<h2 id="2-7가지-핵심-질문-분석-key-analysis">2. 7가지 핵심 질문 분석 (Key Analysis)<a hidden class="anchor" aria-hidden="true" href="#2-7가지-핵심-질문-분석-key-analysis">#</a></h2>
<h3 id="1-what-is-new-in-the-work-기존-연구와의-차별점">1) What is new in the work? (기존 연구와의 차별점)<a hidden class="anchor" aria-hidden="true" href="#1-what-is-new-in-the-work-기존-연구와의-차별점">#</a></h3>
<p>본 연구는 재난 전후 위성 이미지를 공동으로 처리하여 픽셀 수준의 정밀한 피해 지도를 얻는 딥러닝 프레임워크를 제안합니다. 가장 큰 차별점은 <strong>SE-ResNeXt50 인코더</strong>를 통합한 수정된 이중 입력 U-Net 아키텍처를 사용하여 미묘한 구조적 변화를 효과적으로 식별한다는 점입니다. 또한, 픽셀 수준의 예측 결과를 OpenStreetMap 지오메트리와 결합하여 **거리 수준의 영향 분석(Street-Level Impact Analysis)**을 수행하는 후처리 모듈을 도입하여, 단순한 피해 감지를 넘어 재난 대응을 위한 운영 계획(Operational Planning)에 직접 활용 가능한 정보를 제공합니다.</p>
<h3 id="2-why-is-the-work-important-연구의-중요성">2) Why is the work important? (연구의 중요성)<a hidden class="anchor" aria-hidden="true" href="#2-why-is-the-work-important-연구의-중요성">#</a></h3>
<p>이 연구는 재난 발생 직후 응급 대응 우선순위를 정하는 데 필수적인 피해 평가 과정을 자동화하고 가속화합니다. 기존의 수동 평가 방식은 시간이 오래 걸리고 오류 발생 가능성이 높았으나, 이 시스템은 신속하고 일관된 평가 메커니즘을 제공합니다. 특히, 거리 수준의 영향 분석을 통해 피해가 집중된 주요 경로를 식별하는 우선순위 히트맵을 생성함으로써, 구호 기관이 자원을 효율적으로 배분하고 데이터 기반의 재난 관리를 수행할 수 있도록 지원합니다.</p>
<h3 id="3-what-is-the-literature-gap-기존-연구의-한계점">3) What is the literature gap? (기존 연구의 한계점)<a hidden class="anchor" aria-hidden="true" href="#3-what-is-the-literature-gap-기존-연구의-한계점">#</a></h3>
<p>기존의 U-Net 기반 세그멘테이션 및 변화 감지 모델들은 두 가지 주요 한계에 직면했습니다. 첫째, 미묘한 구조적 변화를 정확하게 포착하는 데 어려움을 겪어 정밀한 피해 분류가 어려웠습니다. 둘째, 실제 재난 데이터셋에서 &lsquo;피해 없음&rsquo; 픽셀이 압도적으로 많아 발생하는 **심각한 클래스 불균형(Severe Class Imbalance)**으로 인해, 모델이 &lsquo;주요 피해&rsquo;나 &lsquo;파괴&rsquo;와 같은 희귀한 피해 카테고리를 제대로 감지하지 못했습니다.</p>
<h3 id="4-how-is-the-gap-filled-해결-방안">4) How is the gap filled? (해결 방안)<a hidden class="anchor" aria-hidden="true" href="#4-how-is-the-gap-filled-해결-방안">#</a></h3>
<p>이러한 한계는 세 가지 방식으로 해결되었습니다. 첫째, <strong>이중 입력(Bitemporal) 방식</strong>을 채택하여 재난 전후 이미지를 6채널 입력으로 결합함으로써 구조적 변화를 명확히 학습합니다. 둘째, 인코더로 <strong>SE-ResNeXt50</strong>을 사용하여 채널별 주의 메커니즘(Squeeze-and-Excitation)과 그룹 컨볼루션을 통해 계층적 특징 추출 능력을 높이고 미세한 변화를 식별합니다. 셋째, **클래스 가중치 교차 엔트로피 손실 함수(Class-weighted cross-entropy loss)**를 적용하여 희귀한 피해 클래스에 더 높은 가중치를 부여함으로써 클래스 불균형 문제를 완화하고 주요 피해 감지 성능을 향상시켰습니다.</p>
<h3 id="5-what-is-achieved-with-the-new-method-달성한-성과">5) What is achieved with the new method? (달성한 성과)<a hidden class="anchor" aria-hidden="true" href="#5-what-is-achieved-with-the-new-method-달성한-성과">#</a></h3>
<p>xBD 벤치마크 데이터셋에 대한 실험 결과, 제안된 SE-ResNeXt50 U-Net 모델은 표준 ResNet-50 U-Net 모델 대비 우수한 성능을 달성했습니다 (Table 6.2.1).</p>
<table>
<thead>
<tr>
<th style="text-align:left">Encoder</th>
<th style="text-align:left">mIoU</th>
<th style="text-align:left">Dice</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">ResNet-50 U-Net</td>
<td style="text-align:left">0.69</td>
<td style="text-align:left">0.76</td>
</tr>
<tr>
<td style="text-align:left"><strong>SE-ResNeXt50 U-Net</strong></td>
<td style="text-align:left"><strong>0.74</strong></td>
<td style="text-align:left"><strong>0.81</strong></td>
</tr>
</tbody>
</table>
<p>SE-ResNeXt50 U-Net은 평균 IoU(mIoU)에서 <strong>0.74</strong>를, Dice 점수에서 <strong>0.81</strong>을 기록하며, 표준 모델 대비 성능이 향상되었음을 입증했습니다. 특히, Table 6.1.1에 따르면 가장 중요한 &lsquo;Destroyed&rsquo; 클래스에서 IoU <strong>0.75</strong>, Dice <strong>0.83</strong>의 높은 성능을 보여, 심각한 피해를 정확하게 분류하는 능력을 확인했습니다.</p>
<h3 id="6-what-data-are-used-사용-데이터셋">6) What data are used? (사용 데이터셋)<a hidden class="anchor" aria-hidden="true" href="#6-what-data-are-used-사용-데이터셋">#</a></h3>
<p>주요 학습 및 테스트에는 대규모 재난 매핑 데이터셋인 <strong>xBD</strong>와 <strong>xView2</strong>가 사용되었습니다. 이 데이터셋들은 고해상도 위성 이미지 타일과 포괄적인 폴리곤 주석을 포함하며, 지진, 홍수, 허리케인, 산불 등 광범위한 재난 유형과 다중 클래스 피해 분류를 제공합니다. 건물 수준 주석을 보완하기 위해 <strong>SpaceNet, OpenStreetMap, Open Buildings project</strong>와 같은 지리공간 소스도 활용되어 도로 및 정착지 레이아웃 정보를 제공했습니다.</p>
<h3 id="7-what-are-the-limitations-저자가-언급한-한계점">7) What are the limitations? (저자가 언급한 한계점)<a hidden class="anchor" aria-hidden="true" href="#7-what-are-the-limitations-저자가-언급한-한계점">#</a></h3>
<p>저자들은 다음과 같은 한계점과 향후 연구 방향을 제시했습니다.</p>
<ol>
<li><strong>데이터 의존성:</strong> 현재 모델은 광학 데이터에 의존하므로 구름, 연기, 야간 상황 등 광학 데이터가 부족한 상황에서는 신뢰도가 떨어집니다. (향후 SAR 또는 다중 스펙트럼 이미지 통합 필요)</li>
<li><strong>장거리 구조적 의존성 포착:</strong> 밀집 지역에서 장거리 구조적 의존성을 포착하는 능력을 향상시키기 위해 Transformer 기반 인코더 또는 하이브리드 CNN-ViT 설계가 필요합니다.</li>
<li><strong>거리 수준 분석의 정교화:</strong> 피해가 상호 연결된 도로 네트워크를 통해 어떻게 확산되는지 더 정확하게 모델링하기 위해 거리 수준 분석에 **그래프 신경망(GNNs)**을 통합해야 합니다.</li>
<li><strong>일반화:</strong> 모델의 일반화 능력을 높이기 위해 더 다양한 재난 시나리오를 포함하는 데이터셋 확장이 필요합니다.</li>
</ol>
<hr>
<h2 id="3-아키텍처-및-방법론-architecture--methodology">3. 아키텍처 및 방법론 (Architecture &amp; Methodology)<a hidden class="anchor" aria-hidden="true" href="#3-아키텍처-및-방법론-architecture--methodology">#</a></h2>
<h3 id="figure-분석-메인-아키텍처">Figure 분석: 메인 아키텍처<a hidden class="anchor" aria-hidden="true" href="#figure-분석-메인-아키텍처">#</a></h3>
<p>본 논문에서 제안하는 시스템은 <strong>이중 입력(Bitemporal) U-Net</strong>을 기반으로 하며, 도시 환경(Dense Topographic Features)과 농촌 환경(Sparse Topographic Features) 모두에 적용 가능하도록 설계되었습니다 (Figure 1).</p>
<p><strong>핵심 아키텍처 (Figure 5.4.1): SE-ResNeXt50 U-Net</strong></p>
<ol>
<li><strong>입력:</strong> 재난 전 이미지와 재난 후 이미지를 결합하여 6채널 입력 텐서로 사용합니다.</li>
<li><strong>인코더:</strong> U-Net의 인코더 부분은 <strong>SE-ResNeXt50</strong> 모듈을 사용합니다. 이 인코더는 깊은 주의 모듈(Deep attention modules)과 가중치 공유(Share weight)를 통해 재난 전후 이미지의 특징을 비교하고 구조적 변화를 추출합니다.</li>
<li><strong>디코더 및 스킵 연결:</strong> 인코더에서 추출된 계층적 특징은 디코더로 전달되며, U-Net의 핵심인 **스킵 연결(Skip connection)**을 통해 인코더의 저수준 공간 정보가 디코더로 직접 전달되어 정밀한 픽셀 수준의 세그멘테이션을 가능하게 합니다.</li>
<li><strong>출력:</strong> 픽셀 수준의 7가지 클래스 피해 마스크(No Damage, Minor Damage, Major Damage, Destroyed 등)를 출력합니다.</li>
<li><strong>후처리 (Street-Level Impact Analysis):</strong> 픽셀 수준의 예측 결과를 OpenStreetMap의 도로 세그먼트와 연결하여 거리별 피해 점수($S_j$)를 계산하고 우선순위 히트맵을 생성합니다 (Figure 2 참조).</li>
</ol>
<h3 id="수식-상세">수식 상세<a hidden class="anchor" aria-hidden="true" href="#수식-상세">#</a></h3>
<h4 id="1-입력출력-텐서-형태-inputoutput-tensor-shape">1. 입력/출력 텐서 형태 (Input/Output Tensor Shape)<a hidden class="anchor" aria-hidden="true" href="#1-입력출력-텐서-형태-inputoutput-tensor-shape">#</a></h4>
<ul>
<li>
<p><strong>입력 텐서 ($I_{input}$):</strong> 재난 전 이미지(RGB, 3채널)와 재난 후 이미지(RGB, 3채널)를 채널 축으로 쌓아 6채널로 구성합니다.
$$I_{input} \in \mathbb{R}^{H \times W \times 6}$$
<em>여기서 $H=256, W=256$ (실험에서 사용된 해상도).</em></p>
</li>
<li>
<p><strong>출력 텐서 ($M_{output}$):</strong> 픽셀별 7가지 피해 클래스에 대한 확률 마스크입니다.
$$M_{output} \in \mathbb{R}^{H \times W \times 7}$$</p>
</li>
</ul>
<h4 id="2-손실-함수-loss-function">2. 손실 함수 (Loss Function)<a hidden class="anchor" aria-hidden="true" href="#2-손실-함수-loss-function">#</a></h4>
<p>모델 학습에는 심각한 클래스 불균형(특히 &lsquo;피해 없음&rsquo; 픽셀의 우세)을 해결하기 위해 **클래스 가중치 교차 엔트로피 손실(Class-Weighted Cross-Entropy Loss)**이 사용되었습니다.</p>
<p>$$L = - \frac{1}{N} \sum_{i=1}^{N} \sum_{c=1}^{C} w_c \cdot y_{i,c} \log(\hat{y}_{i,c})$$</p>
<ul>
<li>$N$: 전체 픽셀 수</li>
<li>$C$: 피해 클래스 수 (7개)</li>
<li>$w_c$: 클래스 $c$에 할당된 가중치 (희귀한 피해 클래스에 더 높은 가중치 부여)</li>
<li>$y_{i,c}$: 픽셀 $i$의 클래스 $c$에 대한 실제 레이블 (원-핫 인코딩)</li>
<li>$\hat{y}_{i,c}$: 픽셀 $i$의 클래스 $c$에 대한 예측 확률</li>
</ul>
<h4 id="3-거리-피해-지수-street-damage-index-s_j">3. 거리 피해 지수 (Street Damage Index, $S_j$)<a hidden class="anchor" aria-hidden="true" href="#3-거리-피해-지수-street-damage-index-s_j">#</a></h4>
<p>픽셀 수준의 피해 예측을 거리 수준의 영향으로 변환하기 위해 정규화된 가중치 공식이 사용됩니다 (Section 5.4).</p>
<p>$$S_j = \frac{1}{N_j} \sum_{i \in B_j} w_i \cdot d_i$$</p>
<ul>
<li>$S_j$: 거리 $j$의 피해 점수 (Damage score of the street)</li>
<li>$B_j$: 거리 $j$를 따라 위치한 건물들의 집합 (Set of buildings along street $j$)</li>
<li>$d_i$: 건물 $i$의 피해 수준 (Damage level of building $i$, 1~4 등급)</li>
<li>$w_i$: 건물 면적에 비례하는 가중치 (Weight proportional to building area)</li>
<li>$N_j$: $B_j$에 속한 건물 수</li>
</ul>
<h3 id="vanilla-u-net-비교">Vanilla U-Net 비교<a hidden class="anchor" aria-hidden="true" href="#vanilla-u-net-비교">#</a></h3>
<table>
<thead>
<tr>
<th style="text-align:left">특징</th>
<th style="text-align:left">Vanilla U-Net (Ronneberger et al., 2015)</th>
<th style="text-align:left">Satellite-to-Street (본 연구)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>입력</strong></td>
<td style="text-align:left">단일 이미지 (3채널)</td>
<td style="text-align:left"><strong>이중 입력 (Bitemporal)</strong>: 재난 전후 이미지 스택 (6채널)</td>
</tr>
<tr>
<td style="text-align:left"><strong>인코더</strong></td>
<td style="text-align:left">표준 Convolutional Block</td>
<td style="text-align:left"><strong>SE-ResNeXt50 인코더</strong></td>
</tr>
<tr>
<td style="text-align:left"><strong>핵심 모듈 추가/수정</strong></td>
<td style="text-align:left">없음</td>
<td style="text-align:left"><strong>Squeeze-and-Excitation (SE) 모듈</strong> 및 <strong>그룹 컨볼루션</strong>을 통한 채널별 주의 메커니즘 추가</td>
</tr>
<tr>
<td style="text-align:left"><strong>목표</strong></td>
<td style="text-align:left">생의학 이미지 세그멘테이션</td>
<td style="text-align:left"><strong>이시점 변화 감지</strong> 및 7클래스 피해 분류</td>
</tr>
<tr>
<td style="text-align:left"><strong>손실 함수</strong></td>
<td style="text-align:left">일반적인 교차 엔트로피</td>
<td style="text-align:left"><strong>클래스 가중치 교차 엔트로피 손실</strong> (클래스 불균형 해결)</td>
</tr>
<tr>
<td style="text-align:left"><strong>후처리</strong></td>
<td style="text-align:left">없음</td>
<td style="text-align:left"><strong>거리 수준 영향 분석</strong> (OpenStreetMap 기반)</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="4-태그-제안-tags-suggestion">4. 태그 제안 (Tags Suggestion)<a hidden class="anchor" aria-hidden="true" href="#4-태그-제안-tags-suggestion">#</a></h2>
<ol>
<li><strong>Disaster Assessment (재난 평가)</strong></li>
<li><strong>Bitemporal Analysis (이시점 분석)</strong></li>
<li><strong>Semantic Segmentation (의미론적 분할)</strong></li>
<li><strong>SE-ResNeXt (SE-ResNeXt)</strong></li>
<li><strong>Damage Classification (피해 분류)</strong></li>
</ol>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://hanwo-ol.github.io/Unet_master/tags/disaster-assessment-%EC%9E%AC%EB%82%9C-%ED%8F%89%EA%B0%80/">**Disaster Assessment (재난 평가)**</a></li>
      <li><a href="https://hanwo-ol.github.io/Unet_master/tags/bitemporal-analysis-%EC%9D%B4%EC%8B%9C%EC%A0%90-%EB%B6%84%EC%84%9D/">**Bitemporal Analysis (이시점 분석)**</a></li>
      <li><a href="https://hanwo-ol.github.io/Unet_master/tags/semantic-segmentation-%EC%9D%98%EB%AF%B8%EB%A1%A0%EC%A0%81-%EB%B6%84%ED%95%A0/">**Semantic Segmentation (의미론적 분할)**</a></li>
      <li><a href="https://hanwo-ol.github.io/Unet_master/tags/se-resnext-se-resnext/">**SE-ResNeXt (SE-ResNeXt)**</a></li>
      <li><a href="https://hanwo-ol.github.io/Unet_master/tags/damage-classification-%ED%94%BC%ED%95%B4-%EB%B6%84%EB%A5%98/">**Damage Classification (피해 분류)**</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://hanwo-ol.github.io/Unet_master/posts/cognitive-alpha-mining-via-llm-driven-code-based-evolution/">
    <span class="title">« Prev</span>
    <br>
    <span>Cognitive Alpha Mining via LLM-Driven Code-Based Evolution</span>
  </a>
  <a class="next" href="https://hanwo-ol.github.io/Unet_master/posts/targeted-manipulation-slope-based-attacks-on-financial-time-series-data/">
    <span class="title">Next »</span>
    <br>
    <span>Targeted Manipulation: Slope-Based Attacks on Financial Time-Series Data</span>
  </a>
</nav>


<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share Satellite to Street : Disaster Impact Estimator on twitter"
        href="https://twitter.com/intent/tweet/?text=Satellite%20to%20Street%20%3a%20Disaster%20Impact%20Estimator&amp;url=https%3a%2f%2fhanwo-ol.github.io%2fUnet_master%2fposts%2fsatellite-to-street-disaster-impact-estimator%2f&amp;hashtags=%2a%2aDisasterAssessment%28%ec%9e%ac%eb%82%9c%ed%8f%89%ea%b0%80%29%2a%2a%2c%2a%2aBitemporalAnalysis%28%ec%9d%b4%ec%8b%9c%ec%a0%90%eb%b6%84%ec%84%9d%29%2a%2a%2c%2a%2aSemanticSegmentation%28%ec%9d%98%eb%af%b8%eb%a1%a0%ec%a0%81%eb%b6%84%ed%95%a0%29%2a%2a%2c%2a%2aSE-ResNeXt%28SE-ResNeXt%29%2a%2a%2c%2a%2aDamageClassification%28%ed%94%bc%ed%95%b4%eb%b6%84%eb%a5%98%29%2a%2a">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Satellite to Street : Disaster Impact Estimator on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fhanwo-ol.github.io%2fUnet_master%2fposts%2fsatellite-to-street-disaster-impact-estimator%2f&amp;title=Satellite%20to%20Street%20%3a%20Disaster%20Impact%20Estimator&amp;summary=Satellite%20to%20Street%20%3a%20Disaster%20Impact%20Estimator&amp;source=https%3a%2f%2fhanwo-ol.github.io%2fUnet_master%2fposts%2fsatellite-to-street-disaster-impact-estimator%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Satellite to Street : Disaster Impact Estimator on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2fhanwo-ol.github.io%2fUnet_master%2fposts%2fsatellite-to-street-disaster-impact-estimator%2f&title=Satellite%20to%20Street%20%3a%20Disaster%20Impact%20Estimator">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Satellite to Street : Disaster Impact Estimator on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fhanwo-ol.github.io%2fUnet_master%2fposts%2fsatellite-to-street-disaster-impact-estimator%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Satellite to Street : Disaster Impact Estimator on whatsapp"
        href="https://api.whatsapp.com/send?text=Satellite%20to%20Street%20%3a%20Disaster%20Impact%20Estimator%20-%20https%3a%2f%2fhanwo-ol.github.io%2fUnet_master%2fposts%2fsatellite-to-street-disaster-impact-estimator%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Satellite to Street : Disaster Impact Estimator on telegram"
        href="https://telegram.me/share/url?text=Satellite%20to%20Street%20%3a%20Disaster%20Impact%20Estimator&amp;url=https%3a%2f%2fhanwo-ol.github.io%2fUnet_master%2fposts%2fsatellite-to-street-disaster-impact-estimator%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://hanwo-ol.github.io/Unet_master/">Research Agent Knowledge Base</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
