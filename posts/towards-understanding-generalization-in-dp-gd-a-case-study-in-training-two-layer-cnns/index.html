<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Towards Understanding Generalization in DP-GD: A Case Study in Training Two-Layer CNNs | Research Agent Knowledge Base</title>
<meta name="keywords" content="Differential Privacy (DP), Generalization, DP-GD (Differentially Private Gradient Descent), Implicit Regularization, Convolutional Neural Networks (CNNs)">
<meta name="description" content="Abstract Modern deep learning techniques focus on extracting intricate information from data to achieve accurate predictions. However, the training datasets may be crowdsourced and include sensitive information, such as personal contact details, financial data, and medical records. As a result, there is a growing emphasis on developing privacy-preserving training algorithms for neural networks that maintain good performance while preserving privacy. In this paper, we investigate the generalization and privacy performances of the differentially private gradient descent (DP-GD) algorithm, which is a private variant of the gradient descent (GD) by incorporating additional noise into the gradients during each iteration.">
<meta name="author" content="">
<link rel="canonical" href="https://hanwo-ol.github.io/Unet_master/posts/towards-understanding-generalization-in-dp-gd-a-case-study-in-training-two-layer-cnns/">
<link crossorigin="anonymous" href="/Unet_master/assets/css/stylesheet.62aa25427797f8efd87301a5b69795dc50df2dbe79a5fba0648cc7bb8dbcd7c9.css" integrity="sha256-YqolQneX&#43;O/YcwGltpeV3FDfLb55pfugZIzHu42818k=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/Unet_master/assets/js/highlight.acb54fd32bbc1982428b8850317e45d076b95012730a5936667e6bc21777692a.js" integrity="sha256-rLVP0yu8GYJCi4hQMX5F0Ha5UBJzClk2Zn5rwhd3aSo="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://hanwo-ol.github.io/Unet_master/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://hanwo-ol.github.io/Unet_master/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://hanwo-ol.github.io/Unet_master/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://hanwo-ol.github.io/Unet_master/apple-touch-icon.png">
<link rel="mask-icon" href="https://hanwo-ol.github.io/Unet_master/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><meta property="og:title" content="Towards Understanding Generalization in DP-GD: A Case Study in Training Two-Layer CNNs" />
<meta property="og:description" content="Abstract Modern deep learning techniques focus on extracting intricate information from data to achieve accurate predictions. However, the training datasets may be crowdsourced and include sensitive information, such as personal contact details, financial data, and medical records. As a result, there is a growing emphasis on developing privacy-preserving training algorithms for neural networks that maintain good performance while preserving privacy. In this paper, we investigate the generalization and privacy performances of the differentially private gradient descent (DP-GD) algorithm, which is a private variant of the gradient descent (GD) by incorporating additional noise into the gradients during each iteration." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://hanwo-ol.github.io/Unet_master/posts/towards-understanding-generalization-in-dp-gd-a-case-study-in-training-two-layer-cnns/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2025-11-27T00:00:00+00:00" />
<meta property="article:modified_time" content="2025-11-27T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Towards Understanding Generalization in DP-GD: A Case Study in Training Two-Layer CNNs"/>
<meta name="twitter:description" content="Abstract Modern deep learning techniques focus on extracting intricate information from data to achieve accurate predictions. However, the training datasets may be crowdsourced and include sensitive information, such as personal contact details, financial data, and medical records. As a result, there is a growing emphasis on developing privacy-preserving training algorithms for neural networks that maintain good performance while preserving privacy. In this paper, we investigate the generalization and privacy performances of the differentially private gradient descent (DP-GD) algorithm, which is a private variant of the gradient descent (GD) by incorporating additional noise into the gradients during each iteration."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://hanwo-ol.github.io/Unet_master/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Towards Understanding Generalization in DP-GD: A Case Study in Training Two-Layer CNNs",
      "item": "https://hanwo-ol.github.io/Unet_master/posts/towards-understanding-generalization-in-dp-gd-a-case-study-in-training-two-layer-cnns/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Towards Understanding Generalization in DP-GD: A Case Study in Training Two-Layer CNNs",
  "name": "Towards Understanding Generalization in DP-GD: A Case Study in Training Two-Layer CNNs",
  "description": "Abstract Modern deep learning techniques focus on extracting intricate information from data to achieve accurate predictions. However, the training datasets may be crowdsourced and include sensitive information, such as personal contact details, financial data, and medical records. As a result, there is a growing emphasis on developing privacy-preserving training algorithms for neural networks that maintain good performance while preserving privacy. In this paper, we investigate the generalization and privacy performances of the differentially private gradient descent (DP-GD) algorithm, which is a private variant of the gradient descent (GD) by incorporating additional noise into the gradients during each iteration.",
  "keywords": [
    "Differential Privacy (DP)", "Generalization", "DP-GD (Differentially Private Gradient Descent)", "Implicit Regularization", "Convolutional Neural Networks (CNNs)"
  ],
  "articleBody": "Abstract Modern deep learning techniques focus on extracting intricate information from data to achieve accurate predictions. However, the training datasets may be crowdsourced and include sensitive information, such as personal contact details, financial data, and medical records. As a result, there is a growing emphasis on developing privacy-preserving training algorithms for neural networks that maintain good performance while preserving privacy. In this paper, we investigate the generalization and privacy performances of the differentially private gradient descent (DP-GD) algorithm, which is a private variant of the gradient descent (GD) by incorporating additional noise into the gradients during each iteration. Moreover, we identify a concrete learning task where DP-GD can achieve superior generalization performance compared to GD in training two-layer Huberized ReLU convolutional neural networks (CNNs). Specifically, we demonstrate that, under mild conditions, a small signal-to-noise ratio can result in GD producing training models with poor test accuracy, whereas DP-GD can yield training models with good test accuracy and privacy guarantees if the signal-to-noise ratio is not too small. This indicates that DP-GD has the potential to enhance model performance while ensuring privacy protection in certain learning tasks. Numerical simulations are further conducted to support our theoretical results.\nPDF Download Local PDF View | Arxiv Original\n이 논문은 차분 프라이버시(Differential Privacy, DP)를 적용한 경사 하강법(DP-GD)이 특정 조건에서 표준 경사 하강법(GD)보다 우수한 일반화 성능을 달성할 수 있음을 이론적으로 분석하고 실험적으로 검증합니다.\n1. 요약 (Executive Summary) 이 논문은 2계층 Huberized ReLU CNN을 사용한 특정 이진 분류 작업에서 DP-GD(Differentially Private Gradient Descent)의 일반화(Generalization) 및 프라이버시 성능을 조사합니다.\n프라이버시-유틸리티 상충 관계 극복: 일반적으로 프라이버시 보호를 강화하면 모델 성능이 저하되지만, 이 연구는 특정 학습 작업에서 DP-GD가 표준 GD보다 테스트 정확도 면에서 우수할 수 있음을 이론적으로 입증합니다. 잡음의 정규화 효과: 표준 GD는 신호 대 잡음비(SNR)가 낮을 때 훈련 데이터의 잡음(noise)을 과도하게 기억(memorization)하여 일반화 성능이 저하됩니다. 반면, DP-GD에 반복마다 주입되는 가우시안 잡음은 정규화(regularization) 효과를 발휘하여 잡음 기억을 방지하고 일반화 성능을 향상시킵니다. 핵심 조건: DP-GD가 GD를 능가하는 시나리오는 SNR이 너무 낮지 않으면서도 상대적으로 낮은 ‘중간’ 수준일 때 발생합니다. 동시 달성: 조기 종료(Early Stopping) 기법을 DP-GD에 적용함으로써, 강력한 프라이버시 보장과 경쟁력 있는 일반화 성능을 동시에 달성할 수 있음을 보입니다. 실험적 검증: 수치 시뮬레이션은 이론적 결과를 뒷받침하며, DP-GD가 데이터 잡음에 대한 강건성(robustness)이 GD보다 우수함을 명확히 보여줍니다. 2. 7가지 핵심 질문 분석 (Key Analysis) 1) What is new in the work? (기존 연구와의 차별점) 이 연구의 가장 큰 차별점은 DP-GD가 표준 GD보다 일반화 성능 면에서 우수할 수 있는 구체적인 학습 시나리오를 식별하고 이론적으로 증명했다는 점입니다. 기존 연구들은 DP를 적용하면 유틸리티(성능)가 필연적으로 희생된다는 프라이버시-유틸리티 상충 관계에 초점을 맞췄습니다. 이 논문은 2계층 Huberized ReLU CNN을 사용한 이진 분류 작업에서, DP-GD에 주입된 잡음이 정규화 역할을 하여 GD가 겪는 잡음 기억(noise memorization) 문제를 완화하고 결과적으로 테스트 정확도를 향상시킬 수 있음을 보였습니다.\n2) Why is the work important? (연구의 중요성) 이 연구는 개인 정보 보호 기술이 단순히 성능 저하를 감수하는 비용이 아니라, 특정 조건에서는 모델의 강건성과 일반화 능력을 향상시키는 이점을 제공할 수 있음을 보여줍니다. 이는 의료 진단이나 금융 예측과 같이 정확도와 데이터 기밀성이 모두 중요한 고위험 애플리케이션에서 신뢰할 수 있는(trustworthy) 딥러닝 시스템을 구축하는 데 중요한 통찰력을 제공합니다. 또한, 모델 설계자와 하이퍼파라미터 튜닝 전문가에게 DP 훈련 알고리즘의 잠재력을 활용할 수 있는 구체적인 가이드라인을 제시합니다.\n3) What is the literature gap? (기존 연구의 한계점) 기존 연구들은 DP-GD의 수렴 및 유틸리티 경계에 초점을 맞추었으나, DP가 딥러닝 모델의 일반화에 미치는 영향, 특히 잡음 주입이 정규화 효과를 통해 성능을 향상시킬 수 있는 메커니즘에 대한 심층적인 이론적 이해가 부족했습니다. 특히, 표준 GD가 낮은 SNR 조건에서 훈련 데이터의 잡음을 기억하여 일반화에 실패하는 현상과 DP-GD가 이를 어떻게 극복하는지에 대한 명확한 수학적 분석이 부재했습니다.\n4) How is the gap filled? (해결 방안) 저자들은 이론적 분석의 추적 가능성(tractability)을 위해 2계층 Huberized ReLU CNN 모델과 신호($\\mu$)와 잡음($\\xi$)으로 구성된 단순화된 이진 분류 데이터 분포를 설정했습니다. 핵심 해결 방안은 신호-잡음 분해(Signal-Noise Decomposition) 방법론을 사용하여 GD와 DP-GD의 필터 가중치 업데이트를 신호 학습 계수와 잡음 기억 계수로 분리하여 분석한 것입니다. 이를 통해 GD가 잡음을 기억하는 조건(Theorem 1)과 DP-GD가 신호를 효과적으로 학습하는 조건(Theorem 2, 3)을 엄밀하게 도출했습니다.\n5) What is achieved with the new method? (달성한 성과) 수치 시뮬레이션(Figure 1)을 통해 DP-GD의 일반화 이점을 명확히 입증했습니다.\n잡음 수준 ($\\sigma_p$) 알고리즘 최종 테스트 손실 (Test Loss) 최종 테스트 정확도 (Test Accuracy) GD 대비 성능 변화 0.1 (낮음) GD / DP-GD ~0.0 ~100% 유사한 최적 성능 0.3 (중간) GD ~0.4 ~95% - 0.3 (중간) DP-GD ~0.2 ~100% 손실 0.2 감소, 정확도 5%p 향상 0.5 (높음) GD ~0.6 ~75% 초기화 수준에서 정체 0.5 (높음) DP-GD ~0.2 ~95% 잡음에 대한 강건성 입증 특히 $\\sigma_p=0.3$ 조건에서 DP-GD는 GD보다 테스트 손실을 약 0.2 감소시키고 정확도를 5%p 향상시켜 GD를 능가하는 일반화 성능을 달성했습니다. $\\sigma_p=0.5$ 조건에서는 GD가 초기화 수준(75%)에서 정체하는 반면, DP-GD는 95%의 높은 정확도를 유지하며 잡음에 대한 뛰어난 강건성을 보여주었습니다.\n6) What data are used? (사용 데이터셋) 이 연구는 이론적 분석을 위해 설계된 합성 이진 분류 데이터셋을 사용했습니다 (Definition 1).\n도메인 특성: 데이터 포인트 $x = [x^{(1)T}, x^{(2)T}]^T \\in \\mathbb{R}^{2d}$는 레이블 종속적인 신호($y\\mu$)와 레이블 독립적인 잡음($\\xi$)으로 구성됩니다. 신호-잡음 비율 (SNR): $\\text{SNR} = ||\\mu||_2 / (\\sigma_p \\sqrt{d})$로 정의되며, 실험에서는 고정된 신호 강도($||\\mu||_2 = 1$) 하에 잡음 수준 $\\sigma_p = {0.1, 0.3, 0.5}$를 변화시켜 SNR을 조정했습니다. 레이블: $y \\in {-1, 1}$은 Rademacher 분포를 따릅니다. 7) What are the limitations? (저자가 언급한 한계점) 저자들은 이 연구가 특정 이진 분류 작업과 2계층 Huberized ReLU CNN이라는 제한된 설정에 대한 사례 연구임을 인정합니다. 향후 연구 방향으로는 다음을 제시합니다.\n훈련 역학(training dynamics) 중 프라이버시 보장에 대한 보다 정교한 분석을 도출하는 것. 이론적 결과를 다른 학습 작업(예: 다중 클래스 분류, 회귀)으로 일반화하는 것. 3. 아키텍처 및 방법론 (Architecture \u0026 Methodology) Figure 분석: 2계층 CNN 구조 논문에는 전통적인 아키텍처 다이어그램(Figure 1) 대신 실험 결과 그래프가 제시되어 있습니다. 따라서 아키텍처는 텍스트 기반 정의(Section 3)를 통해 분석합니다.\n이 논문에서 사용된 모델은 **2계층 컨볼루션 신경망(Two-layer CNN)**이며, 활성화 함수로 Huberized ReLU를 사용합니다.\n모델 구조: 모델의 출력 함수 $f(W, x)$는 다음과 같이 정의됩니다: $$f(W, x) = F_{+1}(W_{+1}, x) - F_{-1}(W_{-1}, x)$$ 여기서 $F_j(W_j, x)$는 $j \\in {+1, -1}$에 대한 필터 집합 $W_j$의 출력이며, $m$개의 컨볼루션 필터의 평균으로 계산됩니다: $$F_j(W_j, x) = \\frac{1}{m} \\sum_{r=1}^m [\\sigma(\\langle w_{j,r}, x^{(1)} \\rangle) + \\sigma(\\langle w_{j,r}, x^{(2)} \\rangle)]$$ 입력 데이터 $x \\in \\mathbb{R}^{2d}$는 두 부분 $x = [x^{(1)T}, x^{(2)T}]^T$으로 나뉘며, $x^{(1)}$ 또는 $x^{(2)}$ 중 하나가 신호($y\\mu$)를, 다른 하나가 잡음($\\xi$)을 포함합니다.\n수식 상세 Huberized ReLU Activation Function ($\\sigma(z)$) Huberized ReLU는 분석의 용이성을 위해 사용되는 부드러운(smooth) 활성화 함수입니다. $$\\sigma(z) = \\kappa^{-q} z^q \\cdot \\mathbb{1}{{z \\in [0, \\kappa]}} + \\left(z - \\kappa + \\frac{\\kappa^{q}}{q}\\right) \\cdot \\mathbb{1}{{z \u003e \\kappa}}$$ 여기서 $\\kappa$는 다항식($z^q$)과 선형 함수($z - \\kappa + \\kappa^q/q$) 사이의 경계 임계값이며, $q \\ge 3$입니다.\nEmpirical Cross-Entropy Loss ($L_S(W)$) 훈련 데이터셋 $S = {(x_i, y_i)}{i=1}^n$에 대한 경험적 교차 엔트로피 손실 함수는 다음과 같습니다: $$L_S(W) = \\frac{1}{n} \\sum{i=1}^n l[y_i \\cdot f(W, x_i)]$$ 여기서 $l(t) = \\log(1 + e^{-t})$는 로지스틱 손실(logistic loss)입니다.\nDP-GD Update Rule (Equation 1) DP-GD는 표준 GD에 반복마다 가우시안 잡음 $b_{j,r,t}$를 추가하여 프라이버시를 보장합니다. $$w_{j,r}^{(t+1)} = w_{j,r}^{(t)} - \\eta \\left( \\nabla_{w_{j,r}} L_S(W^{(t)}) + b_{j,r,t} \\right)$$ 여기서 $\\eta$는 학습률(learning rate)이며, 추가된 가우시안 잡음은 $b_{j,r,t} \\sim \\mathcal{N}(0, \\sigma_b^2 I_d)$를 따릅니다.\nGD Update Rule (Equation 2) 표준 GD는 추가 잡음 없이 기울기만을 사용하여 가중치를 업데이트합니다. $$w_{j,r}^{(t+1)} = w_{j,r}^{(t)} - \\eta \\nabla_{w_{j,r}} L_S(W^{(t)})$$\nVanilla U-Net 비교 이 논문은 U-Net 구조를 사용하지 않고 2계층 CNN을 사용하므로, 비교 대상은 표준 GD입니다.\n특징 표준 GD (Vanilla GD) DP-GD (Differentially Private GD) 기울기 업데이트 $\\nabla_{w_{j,r}} L_S(W^{(t)})$ $\\nabla_{w_{j,r}} L_S(W^{(t)}) + b_{j,r,t}$ 추가 모듈/수정 없음 가우시안 잡음 주입 모듈 ($b_{j,r,t} \\sim \\mathcal{N}(0, \\sigma_b^2 I_d)$) 프라이버시 보장 없음 (취약) 차분 프라이버시 보장 일반화 성능 낮은 SNR에서 잡음 기억으로 성능 저하 (Theorem 1) 적절한 SNR에서 잡음이 정규화 역할, 성능 향상 (Theorem 3) DP-GD는 각 반복(iteration)마다 기울기에 독립적인 가우시안 잡음 $b_{j,r,t}$를 추가함으로써, 개별 데이터 포인트가 최종 모델에 미치는 영향을 최소화하여 프라이버시를 보호합니다.\n4. 태그 제안 (Tags Suggestion) Differential Privacy (DP) Generalization DP-GD (Differentially Private Gradient Descent) Implicit Regularization Convolutional Neural Networks (CNNs) ",
  "wordCount" : "1254",
  "inLanguage": "en",
  "datePublished": "2025-11-27T00:00:00Z",
  "dateModified": "2025-11-27T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://hanwo-ol.github.io/Unet_master/posts/towards-understanding-generalization-in-dp-gd-a-case-study-in-training-two-layer-cnns/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Research Agent Knowledge Base",
    "logo": {
      "@type": "ImageObject",
      "url": "https://hanwo-ol.github.io/Unet_master/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://hanwo-ol.github.io/Unet_master/" accesskey="h" title="Research Agent Knowledge Base (Alt + H)">Research Agent Knowledge Base</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://hanwo-ol.github.io/Unet_master/categories/" title="categories">
                    <span>categories</span>
                </a>
            </li>
            <li>
                <a href="https://hanwo-ol.github.io/Unet_master/tags/" title="tags">
                    <span>tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://hanwo-ol.github.io/Unet_master/">Home</a>&nbsp;»&nbsp;<a href="https://hanwo-ol.github.io/Unet_master/posts/">Posts</a></div>
    <h1 class="post-title">
      Towards Understanding Generalization in DP-GD: A Case Study in Training Two-Layer CNNs
    </h1>
    <div class="post-meta">&lt;span title=&#39;2025-11-27 00:00:00 &#43;0000 UTC&#39;&gt;November 27, 2025&lt;/span&gt;&amp;nbsp;·&amp;nbsp;6 min

</div>
  </header> 
  <div class="post-content"><h2 id="abstract">Abstract<a hidden class="anchor" aria-hidden="true" href="#abstract">#</a></h2>
<p>Modern deep learning techniques focus on extracting intricate information from data to achieve accurate predictions. However, the training datasets may be crowdsourced and include sensitive information, such as personal contact details, financial data, and medical records. As a result, there is a growing emphasis on developing privacy-preserving training algorithms for neural networks that maintain good performance while preserving privacy. In this paper, we investigate the generalization and privacy performances of the differentially private gradient descent (DP-GD) algorithm, which is a private variant of the gradient descent (GD) by incorporating additional noise into the gradients during each iteration. Moreover, we identify a concrete learning task where DP-GD can achieve superior generalization performance compared to GD in training two-layer Huberized ReLU convolutional neural networks (CNNs). Specifically, we demonstrate that, under mild conditions, a small signal-to-noise ratio can result in GD producing training models with poor test accuracy, whereas DP-GD can yield training models with good test accuracy and privacy guarantees if the signal-to-noise ratio is not too small. This indicates that DP-GD has the potential to enhance model performance while ensuring privacy protection in certain learning tasks. Numerical simulations are further conducted to support our theoretical results.</p>
<h2 id="pdf-download">PDF Download<a hidden class="anchor" aria-hidden="true" href="#pdf-download">#</a></h2>
<p><a href="//172.22.138.185/Research_pdf/2511.22270v1.pdf">Local PDF View</a> | <a href="http://arxiv.org/abs/2511.22270v1">Arxiv Original</a></p>
<p>이 논문은 차분 프라이버시(Differential Privacy, DP)를 적용한 경사 하강법(DP-GD)이 특정 조건에서 표준 경사 하강법(GD)보다 우수한 일반화 성능을 달성할 수 있음을 이론적으로 분석하고 실험적으로 검증합니다.</p>
<hr>
<h2 id="1-요약-executive-summary">1. 요약 (Executive Summary)<a hidden class="anchor" aria-hidden="true" href="#1-요약-executive-summary">#</a></h2>
<p>이 논문은 2계층 Huberized ReLU CNN을 사용한 특정 이진 분류 작업에서 DP-GD(Differentially Private Gradient Descent)의 일반화(Generalization) 및 프라이버시 성능을 조사합니다.</p>
<ul>
<li><strong>프라이버시-유틸리티 상충 관계 극복:</strong> 일반적으로 프라이버시 보호를 강화하면 모델 성능이 저하되지만, 이 연구는 특정 학습 작업에서 DP-GD가 표준 GD보다 테스트 정확도 면에서 우수할 수 있음을 이론적으로 입증합니다.</li>
<li><strong>잡음의 정규화 효과:</strong> 표준 GD는 신호 대 잡음비(SNR)가 낮을 때 훈련 데이터의 잡음(noise)을 과도하게 기억(memorization)하여 일반화 성능이 저하됩니다. 반면, DP-GD에 반복마다 주입되는 가우시안 잡음은 정규화(regularization) 효과를 발휘하여 잡음 기억을 방지하고 일반화 성능을 향상시킵니다.</li>
<li><strong>핵심 조건:</strong> DP-GD가 GD를 능가하는 시나리오는 SNR이 너무 낮지 않으면서도 상대적으로 낮은 &lsquo;중간&rsquo; 수준일 때 발생합니다.</li>
<li><strong>동시 달성:</strong> 조기 종료(Early Stopping) 기법을 DP-GD에 적용함으로써, 강력한 프라이버시 보장과 경쟁력 있는 일반화 성능을 동시에 달성할 수 있음을 보입니다.</li>
<li><strong>실험적 검증:</strong> 수치 시뮬레이션은 이론적 결과를 뒷받침하며, DP-GD가 데이터 잡음에 대한 강건성(robustness)이 GD보다 우수함을 명확히 보여줍니다.</li>
</ul>
<hr>
<h2 id="2-7가지-핵심-질문-분석-key-analysis">2. 7가지 핵심 질문 분석 (Key Analysis)<a hidden class="anchor" aria-hidden="true" href="#2-7가지-핵심-질문-분석-key-analysis">#</a></h2>
<h3 id="1-what-is-new-in-the-work-기존-연구와의-차별점">1) What is new in the work? (기존 연구와의 차별점)<a hidden class="anchor" aria-hidden="true" href="#1-what-is-new-in-the-work-기존-연구와의-차별점">#</a></h3>
<p>이 연구의 가장 큰 차별점은 DP-GD가 표준 GD보다 일반화 성능 면에서 <em>우수</em>할 수 있는 구체적인 학습 시나리오를 식별하고 이론적으로 증명했다는 점입니다. 기존 연구들은 DP를 적용하면 유틸리티(성능)가 필연적으로 희생된다는 프라이버시-유틸리티 상충 관계에 초점을 맞췄습니다. 이 논문은 2계층 Huberized ReLU CNN을 사용한 이진 분류 작업에서, DP-GD에 주입된 잡음이 정규화 역할을 하여 GD가 겪는 잡음 기억(noise memorization) 문제를 완화하고 결과적으로 테스트 정확도를 향상시킬 수 있음을 보였습니다.</p>
<h3 id="2-why-is-the-work-important-연구의-중요성">2) Why is the work important? (연구의 중요성)<a hidden class="anchor" aria-hidden="true" href="#2-why-is-the-work-important-연구의-중요성">#</a></h3>
<p>이 연구는 개인 정보 보호 기술이 단순히 성능 저하를 감수하는 비용이 아니라, 특정 조건에서는 모델의 강건성과 일반화 능력을 향상시키는 이점을 제공할 수 있음을 보여줍니다. 이는 의료 진단이나 금융 예측과 같이 정확도와 데이터 기밀성이 모두 중요한 고위험 애플리케이션에서 신뢰할 수 있는(trustworthy) 딥러닝 시스템을 구축하는 데 중요한 통찰력을 제공합니다. 또한, 모델 설계자와 하이퍼파라미터 튜닝 전문가에게 DP 훈련 알고리즘의 잠재력을 활용할 수 있는 구체적인 가이드라인을 제시합니다.</p>
<h3 id="3-what-is-the-literature-gap-기존-연구의-한계점">3) What is the literature gap? (기존 연구의 한계점)<a hidden class="anchor" aria-hidden="true" href="#3-what-is-the-literature-gap-기존-연구의-한계점">#</a></h3>
<p>기존 연구들은 DP-GD의 수렴 및 유틸리티 경계에 초점을 맞추었으나, DP가 딥러닝 모델의 <em>일반화</em>에 미치는 영향, 특히 잡음 주입이 정규화 효과를 통해 성능을 향상시킬 수 있는 메커니즘에 대한 심층적인 이론적 이해가 부족했습니다. 특히, 표준 GD가 낮은 SNR 조건에서 훈련 데이터의 잡음을 기억하여 일반화에 실패하는 현상과 DP-GD가 이를 어떻게 극복하는지에 대한 명확한 수학적 분석이 부재했습니다.</p>
<h3 id="4-how-is-the-gap-filled-해결-방안">4) How is the gap filled? (해결 방안)<a hidden class="anchor" aria-hidden="true" href="#4-how-is-the-gap-filled-해결-방안">#</a></h3>
<p>저자들은 이론적 분석의 추적 가능성(tractability)을 위해 2계층 Huberized ReLU CNN 모델과 신호($\mu$)와 잡음($\xi$)으로 구성된 단순화된 이진 분류 데이터 분포를 설정했습니다. 핵심 해결 방안은 <strong>신호-잡음 분해(Signal-Noise Decomposition)</strong> 방법론을 사용하여 GD와 DP-GD의 필터 가중치 업데이트를 신호 학습 계수와 잡음 기억 계수로 분리하여 분석한 것입니다. 이를 통해 GD가 잡음을 기억하는 조건(Theorem 1)과 DP-GD가 신호를 효과적으로 학습하는 조건(Theorem 2, 3)을 엄밀하게 도출했습니다.</p>
<h3 id="5-what-is-achieved-with-the-new-method-달성한-성과">5) What is achieved with the new method? (달성한 성과)<a hidden class="anchor" aria-hidden="true" href="#5-what-is-achieved-with-the-new-method-달성한-성과">#</a></h3>
<p>수치 시뮬레이션(Figure 1)을 통해 DP-GD의 일반화 이점을 명확히 입증했습니다.</p>
<table>
<thead>
<tr>
<th style="text-align:center">잡음 수준 ($\sigma_p$)</th>
<th style="text-align:center">알고리즘</th>
<th style="text-align:center">최종 테스트 손실 (Test Loss)</th>
<th style="text-align:center">최종 테스트 정확도 (Test Accuracy)</th>
<th style="text-align:center">GD 대비 성능 변화</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0.1 (낮음)</td>
<td style="text-align:center">GD / DP-GD</td>
<td style="text-align:center">~0.0</td>
<td style="text-align:center">~100%</td>
<td style="text-align:center">유사한 최적 성능</td>
</tr>
<tr>
<td style="text-align:center"><strong>0.3 (중간)</strong></td>
<td style="text-align:center">GD</td>
<td style="text-align:center">~0.4</td>
<td style="text-align:center">~95%</td>
<td style="text-align:center">-</td>
</tr>
<tr>
<td style="text-align:center"><strong>0.3 (중간)</strong></td>
<td style="text-align:center"><strong>DP-GD</strong></td>
<td style="text-align:center"><strong>~0.2</strong></td>
<td style="text-align:center"><strong>~100%</strong></td>
<td style="text-align:center"><strong>손실 0.2 감소, 정확도 5%p 향상</strong></td>
</tr>
<tr>
<td style="text-align:center"><strong>0.5 (높음)</strong></td>
<td style="text-align:center">GD</td>
<td style="text-align:center">~0.6</td>
<td style="text-align:center">~75%</td>
<td style="text-align:center">초기화 수준에서 정체</td>
</tr>
<tr>
<td style="text-align:center"><strong>0.5 (높음)</strong></td>
<td style="text-align:center"><strong>DP-GD</strong></td>
<td style="text-align:center"><strong>~0.2</strong></td>
<td style="text-align:center"><strong>~95%</strong></td>
<td style="text-align:center"><strong>잡음에 대한 강건성 입증</strong></td>
</tr>
</tbody>
</table>
<p>특히 $\sigma_p=0.3$ 조건에서 DP-GD는 GD보다 테스트 손실을 약 0.2 감소시키고 정확도를 5%p 향상시켜 GD를 능가하는 일반화 성능을 달성했습니다. $\sigma_p=0.5$ 조건에서는 GD가 초기화 수준(75%)에서 정체하는 반면, DP-GD는 95%의 높은 정확도를 유지하며 잡음에 대한 뛰어난 강건성을 보여주었습니다.</p>
<h3 id="6-what-data-are-used-사용-데이터셋">6) What data are used? (사용 데이터셋)<a hidden class="anchor" aria-hidden="true" href="#6-what-data-are-used-사용-데이터셋">#</a></h3>
<p>이 연구는 이론적 분석을 위해 설계된 <strong>합성 이진 분류 데이터셋</strong>을 사용했습니다 (Definition 1).</p>
<ul>
<li><strong>도메인 특성:</strong> 데이터 포인트 $x = [x^{(1)T}, x^{(2)T}]^T \in \mathbb{R}^{2d}$는 레이블 종속적인 신호($y\mu$)와 레이블 독립적인 잡음($\xi$)으로 구성됩니다.</li>
<li><strong>신호-잡음 비율 (SNR):</strong> $\text{SNR} = ||\mu||_2 / (\sigma_p \sqrt{d})$로 정의되며, 실험에서는 고정된 신호 강도($||\mu||_2 = 1$) 하에 잡음 수준 $\sigma_p = {0.1, 0.3, 0.5}$를 변화시켜 SNR을 조정했습니다.</li>
<li><strong>레이블:</strong> $y \in {-1, 1}$은 Rademacher 분포를 따릅니다.</li>
</ul>
<h3 id="7-what-are-the-limitations-저자가-언급한-한계점">7) What are the limitations? (저자가 언급한 한계점)<a hidden class="anchor" aria-hidden="true" href="#7-what-are-the-limitations-저자가-언급한-한계점">#</a></h3>
<p>저자들은 이 연구가 <strong>특정 이진 분류 작업</strong>과 <strong>2계층 Huberized ReLU CNN</strong>이라는 제한된 설정에 대한 사례 연구임을 인정합니다. 향후 연구 방향으로는 다음을 제시합니다.</p>
<ol>
<li>훈련 역학(training dynamics) 중 프라이버시 보장에 대한 보다 정교한 분석을 도출하는 것.</li>
<li>이론적 결과를 다른 학습 작업(예: 다중 클래스 분류, 회귀)으로 일반화하는 것.</li>
</ol>
<hr>
<h2 id="3-아키텍처-및-방법론-architecture--methodology">3. 아키텍처 및 방법론 (Architecture &amp; Methodology)<a hidden class="anchor" aria-hidden="true" href="#3-아키텍처-및-방법론-architecture--methodology">#</a></h2>
<h3 id="figure-분석-2계층-cnn-구조">Figure 분석: 2계층 CNN 구조<a hidden class="anchor" aria-hidden="true" href="#figure-분석-2계층-cnn-구조">#</a></h3>
<p>논문에는 전통적인 아키텍처 다이어그램(Figure 1) 대신 실험 결과 그래프가 제시되어 있습니다. 따라서 아키텍처는 텍스트 기반 정의(Section 3)를 통해 분석합니다.</p>
<p>이 논문에서 사용된 모델은 **2계층 컨볼루션 신경망(Two-layer CNN)**이며, 활성화 함수로 <strong>Huberized ReLU</strong>를 사용합니다.</p>
<p><strong>모델 구조:</strong>
모델의 출력 함수 $f(W, x)$는 다음과 같이 정의됩니다:
$$f(W, x) = F_{+1}(W_{+1}, x) - F_{-1}(W_{-1}, x)$$
여기서 $F_j(W_j, x)$는 $j \in {+1, -1}$에 대한 필터 집합 $W_j$의 출력이며, $m$개의 컨볼루션 필터의 평균으로 계산됩니다:
$$F_j(W_j, x) = \frac{1}{m} \sum_{r=1}^m [\sigma(\langle w_{j,r}, x^{(1)} \rangle) + \sigma(\langle w_{j,r}, x^{(2)} \rangle)]$$
입력 데이터 $x \in \mathbb{R}^{2d}$는 두 부분 $x = [x^{(1)T}, x^{(2)T}]^T$으로 나뉘며, $x^{(1)}$ 또는 $x^{(2)}$ 중 하나가 신호($y\mu$)를, 다른 하나가 잡음($\xi$)을 포함합니다.</p>
<h3 id="수식-상세">수식 상세<a hidden class="anchor" aria-hidden="true" href="#수식-상세">#</a></h3>
<h4 id="huberized-relu-activation-function-sigmaz">Huberized ReLU Activation Function ($\sigma(z)$)<a hidden class="anchor" aria-hidden="true" href="#huberized-relu-activation-function-sigmaz">#</a></h4>
<p>Huberized ReLU는 분석의 용이성을 위해 사용되는 부드러운(smooth) 활성화 함수입니다.
$$\sigma(z) = \kappa^{-q} z^q \cdot \mathbb{1}<em>{{z \in [0, \kappa]}} + \left(z - \kappa + \frac{\kappa^{q}}{q}\right) \cdot \mathbb{1}</em>{{z &gt; \kappa}}$$
여기서 $\kappa$는 다항식($z^q$)과 선형 함수($z - \kappa + \kappa^q/q$) 사이의 경계 임계값이며, $q \ge 3$입니다.</p>
<h4 id="empirical-cross-entropy-loss-l_sw">Empirical Cross-Entropy Loss ($L_S(W)$)<a hidden class="anchor" aria-hidden="true" href="#empirical-cross-entropy-loss-l_sw">#</a></h4>
<p>훈련 데이터셋 $S = {(x_i, y_i)}<em>{i=1}^n$에 대한 경험적 교차 엔트로피 손실 함수는 다음과 같습니다:
$$L_S(W) = \frac{1}{n} \sum</em>{i=1}^n l[y_i \cdot f(W, x_i)]$$
여기서 $l(t) = \log(1 + e^{-t})$는 로지스틱 손실(logistic loss)입니다.</p>
<h4 id="dp-gd-update-rule-equation-1">DP-GD Update Rule (Equation 1)<a hidden class="anchor" aria-hidden="true" href="#dp-gd-update-rule-equation-1">#</a></h4>
<p>DP-GD는 표준 GD에 반복마다 가우시안 잡음 $b_{j,r,t}$를 추가하여 프라이버시를 보장합니다.
$$w_{j,r}^{(t+1)} = w_{j,r}^{(t)} - \eta \left( \nabla_{w_{j,r}} L_S(W^{(t)}) + b_{j,r,t} \right)$$
여기서 $\eta$는 학습률(learning rate)이며, 추가된 가우시안 잡음은 $b_{j,r,t} \sim \mathcal{N}(0, \sigma_b^2 I_d)$를 따릅니다.</p>
<h4 id="gd-update-rule-equation-2">GD Update Rule (Equation 2)<a hidden class="anchor" aria-hidden="true" href="#gd-update-rule-equation-2">#</a></h4>
<p>표준 GD는 추가 잡음 없이 기울기만을 사용하여 가중치를 업데이트합니다.
$$w_{j,r}^{(t+1)} = w_{j,r}^{(t)} - \eta \nabla_{w_{j,r}} L_S(W^{(t)})$$</p>
<h3 id="vanilla-u-net-비교">Vanilla U-Net 비교<a hidden class="anchor" aria-hidden="true" href="#vanilla-u-net-비교">#</a></h3>
<p>이 논문은 U-Net 구조를 사용하지 않고 2계층 CNN을 사용하므로, 비교 대상은 <strong>표준 GD</strong>입니다.</p>
<table>
<thead>
<tr>
<th style="text-align:left">특징</th>
<th style="text-align:left">표준 GD (Vanilla GD)</th>
<th style="text-align:left">DP-GD (Differentially Private GD)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>기울기 업데이트</strong></td>
<td style="text-align:left">$\nabla_{w_{j,r}} L_S(W^{(t)})$</td>
<td style="text-align:left">$\nabla_{w_{j,r}} L_S(W^{(t)}) + b_{j,r,t}$</td>
</tr>
<tr>
<td style="text-align:left"><strong>추가 모듈/수정</strong></td>
<td style="text-align:left">없음</td>
<td style="text-align:left"><strong>가우시안 잡음 주입 모듈</strong> ($b_{j,r,t} \sim \mathcal{N}(0, \sigma_b^2 I_d)$)</td>
</tr>
<tr>
<td style="text-align:left"><strong>프라이버시 보장</strong></td>
<td style="text-align:left">없음 (취약)</td>
<td style="text-align:left">차분 프라이버시 보장</td>
</tr>
<tr>
<td style="text-align:left"><strong>일반화 성능</strong></td>
<td style="text-align:left">낮은 SNR에서 잡음 기억으로 성능 저하 (Theorem 1)</td>
<td style="text-align:left">적절한 SNR에서 잡음이 정규화 역할, 성능 향상 (Theorem 3)</td>
</tr>
</tbody>
</table>
<p>DP-GD는 각 반복(iteration)마다 기울기에 독립적인 가우시안 잡음 $b_{j,r,t}$를 추가함으로써, 개별 데이터 포인트가 최종 모델에 미치는 영향을 최소화하여 프라이버시를 보호합니다.</p>
<hr>
<h2 id="4-태그-제안-tags-suggestion">4. 태그 제안 (Tags Suggestion)<a hidden class="anchor" aria-hidden="true" href="#4-태그-제안-tags-suggestion">#</a></h2>
<ol>
<li>Differential Privacy (DP)</li>
<li>Generalization</li>
<li>DP-GD (Differentially Private Gradient Descent)</li>
<li>Implicit Regularization</li>
<li>Convolutional Neural Networks (CNNs)</li>
</ol>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://hanwo-ol.github.io/Unet_master/tags/differential-privacy-dp/">Differential Privacy (DP)</a></li>
      <li><a href="https://hanwo-ol.github.io/Unet_master/tags/generalization/">Generalization</a></li>
      <li><a href="https://hanwo-ol.github.io/Unet_master/tags/dp-gd-differentially-private-gradient-descent/">DP-GD (Differentially Private Gradient Descent)</a></li>
      <li><a href="https://hanwo-ol.github.io/Unet_master/tags/implicit-regularization/">Implicit Regularization</a></li>
      <li><a href="https://hanwo-ol.github.io/Unet_master/tags/convolutional-neural-networks-cnns/">Convolutional Neural Networks (CNNs)</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://hanwo-ol.github.io/Unet_master/posts/spatiotemporal-satellite-image-downscaling-with-transfer-encoders-and-autoregressive-generative-models/">
    <span class="title">« Prev</span>
    <br>
    <span>Spatiotemporal Satellite Image Downscaling with Transfer Encoders and Autoregressive Generative Models</span>
  </a>
  <a class="next" href="https://hanwo-ol.github.io/Unet_master/posts/integrating-lstm-networks-with-neural-levy-processes-for-financial-forecasting/">
    <span class="title">Next »</span>
    <br>
    <span>Integrating LSTM Networks with Neural Levy Processes for Financial Forecasting</span>
  </a>
</nav>


<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share Towards Understanding Generalization in DP-GD: A Case Study in Training Two-Layer CNNs on twitter"
        href="https://twitter.com/intent/tweet/?text=Towards%20Understanding%20Generalization%20in%20DP-GD%3a%20A%20Case%20Study%20in%20Training%20Two-Layer%20CNNs&amp;url=https%3a%2f%2fhanwo-ol.github.io%2fUnet_master%2fposts%2ftowards-understanding-generalization-in-dp-gd-a-case-study-in-training-two-layer-cnns%2f&amp;hashtags=DifferentialPrivacy%28DP%29%2cGeneralization%2cDP-GD%28DifferentiallyPrivateGradientDescent%29%2cImplicitRegularization%2cConvolutionalNeuralNetworks%28CNNs%29">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Towards Understanding Generalization in DP-GD: A Case Study in Training Two-Layer CNNs on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fhanwo-ol.github.io%2fUnet_master%2fposts%2ftowards-understanding-generalization-in-dp-gd-a-case-study-in-training-two-layer-cnns%2f&amp;title=Towards%20Understanding%20Generalization%20in%20DP-GD%3a%20A%20Case%20Study%20in%20Training%20Two-Layer%20CNNs&amp;summary=Towards%20Understanding%20Generalization%20in%20DP-GD%3a%20A%20Case%20Study%20in%20Training%20Two-Layer%20CNNs&amp;source=https%3a%2f%2fhanwo-ol.github.io%2fUnet_master%2fposts%2ftowards-understanding-generalization-in-dp-gd-a-case-study-in-training-two-layer-cnns%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Towards Understanding Generalization in DP-GD: A Case Study in Training Two-Layer CNNs on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2fhanwo-ol.github.io%2fUnet_master%2fposts%2ftowards-understanding-generalization-in-dp-gd-a-case-study-in-training-two-layer-cnns%2f&title=Towards%20Understanding%20Generalization%20in%20DP-GD%3a%20A%20Case%20Study%20in%20Training%20Two-Layer%20CNNs">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Towards Understanding Generalization in DP-GD: A Case Study in Training Two-Layer CNNs on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fhanwo-ol.github.io%2fUnet_master%2fposts%2ftowards-understanding-generalization-in-dp-gd-a-case-study-in-training-two-layer-cnns%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Towards Understanding Generalization in DP-GD: A Case Study in Training Two-Layer CNNs on whatsapp"
        href="https://api.whatsapp.com/send?text=Towards%20Understanding%20Generalization%20in%20DP-GD%3a%20A%20Case%20Study%20in%20Training%20Two-Layer%20CNNs%20-%20https%3a%2f%2fhanwo-ol.github.io%2fUnet_master%2fposts%2ftowards-understanding-generalization-in-dp-gd-a-case-study-in-training-two-layer-cnns%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Towards Understanding Generalization in DP-GD: A Case Study in Training Two-Layer CNNs on telegram"
        href="https://telegram.me/share/url?text=Towards%20Understanding%20Generalization%20in%20DP-GD%3a%20A%20Case%20Study%20in%20Training%20Two-Layer%20CNNs&amp;url=https%3a%2f%2fhanwo-ol.github.io%2fUnet_master%2fposts%2ftowards-understanding-generalization-in-dp-gd-a-case-study-in-training-two-layer-cnns%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://hanwo-ol.github.io/Unet_master/">Research Agent Knowledge Base</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
