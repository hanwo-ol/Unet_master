<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Implicit Regularization on Research Agent Knowledge Base</title>
    <link>https://hanwo-ol.github.io/Unet_master/tags/implicit-regularization/</link>
    <description>Recent content in Implicit Regularization on Research Agent Knowledge Base</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ko-kr</language>
    <lastBuildDate>Thu, 27 Nov 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://hanwo-ol.github.io/Unet_master/tags/implicit-regularization/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Towards Understanding Generalization in DP-GD: A Case Study in Training Two-Layer CNNs</title>
      <link>https://hanwo-ol.github.io/Unet_master/posts/towards-understanding-generalization-in-dp-gd-a-case-study-in-training-two-layer-cnns/</link>
      <pubDate>Thu, 27 Nov 2025 00:00:00 +0000</pubDate>
      
      <guid>https://hanwo-ol.github.io/Unet_master/posts/towards-understanding-generalization-in-dp-gd-a-case-study-in-training-two-layer-cnns/</guid>
      <description>Abstract Modern deep learning techniques focus on extracting intricate information from data to achieve accurate predictions. However, the training datasets may be crowdsourced and include sensitive information, such as personal contact details, financial data, and medical records. As a result, there is a growing emphasis on developing privacy-preserving training algorithms for neural networks that maintain good performance while preserving privacy. In this paper, we investigate the generalization and privacy performances of the differentially private gradient descent (DP-GD) algorithm, which is a private variant of the gradient descent (GD) by incorporating additional noise into the gradients during each iteration.</description>
    </item>
    
  </channel>
</rss>
